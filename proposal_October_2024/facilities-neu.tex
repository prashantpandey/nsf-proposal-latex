\section*{Facilities, Equipment, and Other Resources}

PI Pandey's research group will use high-performance computing infrastructure at Northeastern University. PI Pandey is also an Affiliate Scientist with Lawrence Berkeley National Lab and his research group also uses NERSC computing facilities at Lawrence Berkeley National lab.

\para{SALT systems Lab (Pandey’s research lab)}\\
\noindent
Northeastern University will provide a dedicated lab space for Dr. Pandey’s research group and students. PI Pandey's research currently has 3 PhD students and 1 MS student. PI Pandey manages local compute/storage infrastructure for software development and testing. It contains 10 state-of-the-art computer workstations for software development including machine learning research with GPU acceleration, a high-end rack machine with 1TB of RAM and 10TB of SSD storage for software development and evaluating prototype software. 

\para{College Facilities}\\
\noindent
The Khoury College of Computer and Sciences at Northeastern University provides exclusive computing resources for both faculty research and teaching purposes. These facilities are constantly being upgraded to keep pace with developments in the computer industry. Our infrastructure is managed by the Khoury Systems team that is dedicated to computing support for the college and consists of five experienced Linux, Windows, Network, and Virtualization administrators with a dedicated Service Desk.

The College has three laboratories for student computation housing a mix of managed Linux and Windows workstations. One of these laboratories, 102WVH, accommodates over 100 Khoury students who will rely upon on the College’s new Virtual Desktop platform utilizing VMware Horizon. The new virtual desktop environment provides both Linux and Windows desktops, expanding the desktop computing capabilities of the college. 

Our computing facilities are connected via a layer-3 switched ethernet gigabit network. Our research and teaching environments are serviced by a cadre of dedicated data servers. Our infrastructure includes a pair of NetApp filers with 98TB storage for home directories, administrative and projects shares, and research volumes, and a Pure FlashArray//M10 all-flash storage array provides high speed storage. The server room contains a large deployment of high-end Dell servers including a pair SSH login hosts with dual 2.4 Intel Xeon processors and 64GB memory. The college exclusively manages core services including identity management, email, web servers, configuration management, and networking services. We maintain two distinct VMWare vSphere virtualization clusters for research and teaching purposes, currently totaling 12 hosts, with that number increasing each year. Our managed Lab, Desktop, and Server environment consists of CentOS Linux 7 (servers and desktops), Windows (desktops), and Mac (laptops).

Together with our core infrastructure, Khoury Systems provides enterprise-grade server hosting in comprising of network, power, and cooling services. Researchers may utilize these capabilities by staging personal or research group computing systems in the Khoury Systems data center. Khoury Systems will also provide guidance in vendor selection, defining equipment specifications, and assisting in the purchasing process for all computing hardware requirements. Additionally, centrally managed applications such as GitHub Enterprise, WordPress Multisite, and Confluence Wiki are provided as tools for both research and teaching purposes. 

\para{Information Technology Services - Research Computing}
\noindent
Northeastern ITS Research Computing provides high-end research computing resources to all Northeastern University researchers and manages Northeaster’s involvement in the Massachusetts Green High Performance Computing Center facility. These resources include centralized high-performance computing (HPC) clusters and storage, visualization, software, high-level technical and scientific user support, education and training. Researchers can leverage these for both HPC and non-HPC research computing use, development and funding requirements. 

As of February 2020, the shared Discovery cluster provides secure access to over 50,000 CPU cores and over 525 GPUs to all Northeastern faculty and students free of charge. Hardware currently available for research consists of a combination of Intel Xeon (Cascadelake, Skylake, Broadwell, Haswell, Sandybridge, and Ivybridge) and AMD (Zen, Zen2) CPU microarchitectures. Additionally, a selection of NVIDIA Pascal (P100), Volta (V100), Turing (T4), Ampere (A100), and Hopper (H100) GPUs. Discovery is connected to the university network over 10 Gbps Ethernet (GbE) for high-speed data transfer, and Discovery provides 6 PB of available storage on a high-performance file system. Compute nodes are connected with either 10 GbE or high data rate InfiniBand (200 Gbps or 100 Gbps), supporting all types and scales of computational workloads.

Project storage and other services are available at a recurring charge.  A dedicated team of research computing staff (and several graduate students) manages the environment and supports researchers in their use of high-performance computing for research and discovery.  
Researchers who need access to their own dedicated computational queues may do so via the ‘buy in’ service, wherein ITSRC facilitates, integrates and fully manages computational equipment purchased by the researcher as part of the Discovery cluster, for no additional charge.  


% \para{University of Utah}\\
% The University of Utah provides several computing facilities for instructional and research use. The campus network backbone is a 40+Gbps network with a 100 Gbps research DMZ\@. The campus attaches via redundant 10 Gbps links to the Utah Education Network (UEN) which provides commodity internet and research connectivity. UEN maintains multiple gigabits of commodity from various carriers at strategic points throughout the state. For research connectivity, UEN connects via 10 Gbps directly with the 100 Gbps backbone of Internet2, both at the Salt Lake Level3 PoP. The University of Utah’s Kahlert School of Computing attaches via redundant 10 Gbps connections to the campus backbone routed via OSPF\@; this provides desktop connections with 1 Gbps ethernet.

% \para{Kahlert School of Computing}\\
% The Kahlert School of Computing’s computing infrastructure supplies many centralized services, including shared disk space (100 Terabytes), time, web/cgi/php, s/ftp, firewall, backups, printing resources, authentication (AD/LDAP/NIS), vpn, ssh/interactive servers, door lock access, and email. The core of the server infrastructure runs on VMware’s Enterprise virtualization products. Most services run on VM-Linux-based hosts, with some additional services being served from Windows machines. The Kahlert School of Computing’s core instructional computing facility is the Computer Aided Design and Engineering (CADE) Lab. The CADE Lab includes approximately 75 personal computers running Centos 7.2 Linux with the 3.10 kernel, deployed on hardware equipped with an 3.60GHz Intel Core i7-4790 Processor, 32~GB DDR3 1600~MHz overclocked RAM, GeForce GTX 770/970 with 4~GB of memory, and a 240~GB Intel 540 Solid State Drive.

% \para{Center for High Performance Computing}\\
% The University of Utah’s core research computing facility is the Center for High Performance Computing (CHPC), which supports the deployment and operation of large-scale and high performance computational resources, while also facilitating use of these resources through advanced user support and training. The CHPC also serves as an expert team to broadly support the diverse research computing needs on campus, in- cluding support for big data, big data movement, data analytics, security, virtual machines, Windows science application servers, protected environments for data mining and analysis of protected health information, and advanced networking. The CHPC manages over 22,000 cores and over 13 PB of RAID configured spinning disks. CHPC also leverages the national cyberinfrastructure for resources and training, including serving as partners in the ACI-REF (Advanced Cyberinfrastructure Research and Education Facilitators) program and as a member of RMACC (Rocky Mountain Advanced Computing Consortium).

\para{Lawrence Berkeley National Laboratory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Lawrence Berkeley National Laboratory (Berkeley Lab) is the leading provider of computing and networking resources supporting the DOE Office of Science's research mission. Berkeley Lab researchers have access to leading-edge computing platforms and services at the National Energy Research Scientific Computing Center (NERSC) and have 100 Gbps connectivity to other national labs and institutions via ESnet, DOE's Energy Sciences Network, both of which are managed by Berkeley Lab. The Lab also manages several departmental clusters.

\para{NERSC}

NERSC is the mission high-performance computing facility for the DOE's Office of Science, supporting large-scale simulations and a growing workload in data analysis from the DOE's experimental and observational facilities. Managed by DOE's SC Advanced Scientific Computing Research program and operated by Berkeley Lab, in 2021 NERSC served more than 8,000 scientists, working on about 900 research projects, with users from all 50 states and Washington D.C. that span the range of SC scientific disciplines. NERSC has a significant impact on scientific research; e.g., proposals for 2020 allocations cited over 2,000 distinct refereed publications that used NERSC\@. In 2020, 61\% of NERSC users came from universities, 30\% from DOE national laboratories, and the remainder from other government laboratories, industry, and nonprofits.

\begin{figure}[ht]
 \begin{center}
    \includegraphics[width=0.5\textwidth]
    {images/perlmutter.jpg}
 \end{center}
\end{figure}

In addition to providing world-class supercomputers, NERSC offers expert support to ensure that its users make the most efficient and effective use of the facility's resources. NERSC staff help users opt
imize their applications for advanced computing architectures, and they engage with users from experimental facilities to run complex workflows on NERSC systems. Increasingly, NERSC staff aid users in im
plementing advanced analytics and machine learning capabilities into their applications and workflows.

In 2021 NERSC unveiled its newest flagship supercomputer, Perlmutter, a Cray Shasta system based on NVIDIA A100 GPUs with new tensor core technology, AMD EPYC CPUs, and the HPE Cray ``Slingshot'' high-speed interconnect. The system is named in honor of Saul Perlmutter, an astrophysicist at Berkeley Lab and a professor of physics at UC Berkeley who shared the 2011 Nobel Prize in Physics for his contribut
ions to research showing that the expansion of the universe is accelerating. Perlmutter is ranked as the 5th most powerful supercomputer in the world as of November 2021 and includes several innovations
designed to meet the diverse computational and data analysis needs of NERSC's user base and speed their scientific productivity, including  an all-flash scratch filesystem. Developed by Cray to accelerat
e I/O, the 30-petabyte Lustre filesystem will move data at a rate of more than 4 terabytes/sec.

\begin{figure}[ht]
 \begin{center}
    \includegraphics[width=0.6\textwidth]
    {images/cori.png}
 \end{center}
\end{figure}

NERSC also hosts an Intel-based Cray XC40 with a peak performance of 30 petaflop/s. Named ``Cori'' in honor of biochemist Gerty Cori, the first American woman to receive a Nobel Prize in science, the sys
tem delivered over 9 billion core hours in 2020 to the DOE SC user community and has several features that benefit data-intensive science. The system includes more than 1,600 Intel Xeon ``Haswell'' compu
te nodes and over 9,300 nodes of the Intel Xeon Phi processors (code-named Knights Landing, or KNL for short).  Cori uses the Cray Aries interconnect, a dragonfly network topology that provides scalable
bandwidth.

NERSC has several storage platforms, including the Community Filesystem, a global file system available on all NERSC computational systems. It offers users a platform for collaboration and data sharing.
NERSC systems are also connected to a High Performance Storage System (HPSS) for archival storage. NERSC's HPSS system currently contains more than 200 petabytes, making it one of the world's largest unc
lassified archival storage systems.

\para{ESnet}

\begin{figure}[ht]
  \begin{center}
    \includegraphics[width=0.95\textwidth]
    {images/esnet.jpeg}
  \end{center}
\end{figure}

ESnet interconnects the DOE's national laboratory system, dozens
of other DOE sites, and $\sim$200 research and commercial networks around
the world, enabling tens of thousands of scientists at DOE laboratories
and academic institutions across the country to transfer vast data
streams and access distributed research and computing resources in
real-time. ESnet achieves this by providing high-bandwidth, reliable
connections that enable many thousands of the nation's scientists
to collaborate on some of the world's most important scientific
challenges including energy, biosciences, materials, and the origins
of the universe. ESnet operates what is essentially the Department's
circulatory system for the movement of large-scale scientific data,
providing real-time networking to many thousands of users across
the entire DOE complex.

Access to Berkeley Lab's computational and experimental facilities
from anywhere in the U.S.~or the world is provided by ESnet. ESnet
has extended its connectivity to Europe with four 100-Gbps connections.


