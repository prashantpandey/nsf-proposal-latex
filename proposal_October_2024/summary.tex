%!TEX root =  proposal.tex

%% Don't actually need this - will submit txt.  Just want to estimate actual space
\begin{center}

\bf
\Large
Collaborative Research: CSR: Medium: Dynamic and Distributed Data Structures on the GPU

% \medskip
\small
Lead PI: Mart\'{i}n Farach-Colton (NYU) \\
Co-PIs: Michael A. Bender (SBU), John Owens (UC Davis), Prashant Pandey (Northeastern), Rob Patro (UMD)
\end{center}

\vspace{-0.3cm}

\noindent \textbf{\large Overview:}\\
Both the volume and variety of genomic sequencing data has been increasing at an ever faster rate driven by high-throughput sequencing (HTS) technologies. 
% Genomic sequencing data is being generated faster than ever due to modern sequencing technologies.
% , which is already producing petabyte-scale datasets
Many applications in computational biology (\kmer analysis, single-cell transcriptomics, variant calling, transcript identification, raw sequence search, taxonomic classification, genome and metagenomic assembly, and pangenomics) require processing raw sequencing data at the petabyte scales.
%
These computational-biology applications (and numerous others) use a set of common data structures to perform many data-analysis tasks, and their performance is bottlenecked by the data-structure and algorithm performance.

This project aims to develop high-performance and scalable data analysis-pipelines for computational-biology applications. Specifically, we aim to develop new parallel and distributed data structures and algorithms that sit at the heart of many computational biology data processing tasks. These new data structures and algorithms will have wide applicability in computational biology applications and beyond.
%
We will develop new \textbf{algorithmic theory} to design scalable data structures and new \textbf{systems} that implement our solutions in a scale-up manner, both on CPUs and GPUs. We will develop a new framework to distribute our solutions in an \textbf{HPC} environment, so they scale out to clusters of GPUs; and finally we will validate our solutions on \textbf{computational biology} workloads.
%
Specifically, the data structures we propose to study are: \textbf{filters}, \textbf{sketches}, \textbf{hash tables}, \textbf{string indexes}, and related data structures.  We will validate our solutions on \textbf{large-scale raw sequence search}, \textbf{single-cell preprocessing}, \textbf{taxonomic classification for metagenomic data}, \textbf{pangenomic indexing and analysis}, \textbf{metagenome assembly}, and others.

\noindent \textbf{\large Keywords:} Data structures, algorithms, computational biology, GPUs, high-performance computing (HPC).

\noindent \textbf{\large Intellectual Merit:}\\
If successful we will be able to perform complex data analyses to answer biological questions on terabyte- and petabyte-scale datasets. For example, raw sequencing data from sequence read archive (SRA) is already at petabyte scale, metagenomic data from Western Arctic and Rhizo are in terabytes, and population-scale pangenomic data from the 100,000 Genome Project contains the genomes of $\approx85$K individuals. Existing data structures and software tools fail to scale to these data sizes, limiting the value of these essential public resources, and hampering critical data re-analysis in light of new and improved computational methods.

The projectâ€™s novelties are: a vertical-stack approach spanning \textbf{theory and algorithms}: highly concurrent, dynamic, and distributed data structures, \textbf{systems}: scale up using GPU acceleration, \textbf{high-performance computing}: scale out using distributed data structures, and \textbf{applications}: computational biology applications; new parallel and distributed data structures and algorithms to exploit the massive compute on GPUs applicable to other application domains; and an API for developers to quickly and seamlessly integrate high-performance and scalable data structures in applications.
%
Our team constitutes a highly interdisciplinary array of researchers across four focus areas: applications (computational biology), theory and algorithms, systems, and high-performance computing. The team is taking a holistic theory/systems/HPC/applications co-design approach to explore four tightly interconnected research modules. These research modules are structured from bottom-up across the computing stack.

\noindent \textbf{\large Broader Impacts: }\\
The primary broader impact of this work is to enhance the capability of bioinformatics applications to perform computations at the largest scales. Both accelerated computation (allowing quicker feedback and more experiments) and larger computation potentially accelerate the process of scientific discovery. Second, we expect that a high-quality library of distributed, scalable GPU data structures will find widespread utility in other application domains.
%
Beyond technical broader impacts, we propose outreach impacts through tutorial on using our data structure library in computational biology conferences and proposing week-long research seminars like Dagstuhl that we hope will bridge the gaps between computational biologists, CS theorists, and CS systems researchers.
