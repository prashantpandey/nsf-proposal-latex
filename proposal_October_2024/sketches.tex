%!TEX root =  proposal.tex

\section{Sketches}

\mfc{this tool is so general, and this application is so specific by comparison}

Measuring sequence similarity is at the core of many algorithms in  computational biology~\cite{Myers2000, Langmead2012,Li2010}. Computing sequence similarity via optimal alignment is computationally intensive and requires an $O(n^2)$-time solution for two sequences of length $n$, in the worst case~\cite{backurs2015edit}.  For highly similar sequences (or where the maximum allowable edit distance is bounded), faster algorithms~\cite{MarcoSola2023} are known, and some have been implemented on the GPU~\cite{AguadoPuig2023}. However, due to the sheer size of datasets available today in terms of both complete and draft assemblies (e.g., metagenomic assemblies in RefSeq~\cite{RefSeq}) and raw sequences in SRA~\cite{SRA}, computing sequence similarities using traditional algorithms --- even the most efficient variants --- is not feasible.

Recently, researchers have employed dimensionality-reduction techniques, such as Minhash~\cite{Broder97}, ordered-min hash~\cite{Marais2019}, and HyperLogLog~\cite{Baker2019} to include a pairwise-mutation distance and P-value-significance test, enabling  efficient clustering and searching in massive sequence collections~\cite{Mitzenmacher2014,ondov2016mash,Marais2019,Baker2019}. PI Pandey developed a new locality sensitive hashing (LSH) method, called Order Min Hash (OMH)~\cite{Marais2019}, for  edit distance. This method is a refinement of the minHash LSH used to approximate the Jaccard similarity, in that OMH is sensitive not only to the \kmer contents of the sequences but also to the relative order of the \kmers in the sequences. PI Pandey presented theoretical guarantees of the OMH as a gapped LSH\@.

One of the primary bottlenecks of these algorithms is the high computational cost of their linear-sketching subroutines. Existing sketching techniques~\cite{ondov2016mash,Marais2019} are based on hashing and offer poor locality. This makes them harder to scale up using GPUs.

\begin{rproblem}
Develop sketching algorithms that can replace hashing with batch sorting and achieve high performance on GPUs.
\end{rproblem}

Our approach is to develop algorithmic techniques where we can compute sketches by pre-computing and sorting the tokens (\kmers in the comp. bio. case) in a hash order. Doing this will help preserve locality and achieve massive parallelism on GPUs. PI Pandey has explored similar techniques in the past to preserve the order of \kmers in the sequence~\cite{Marais2019}.

\begin{rproblem}
Develop data structures for indexing and searching low-dimensional embeddings (based on minhash).  Test them on raw genomic and metagenomic data.
\end{rproblem}

\mfc{every problem needs to be rewritten so that it's Build x.  Test is on comp bio.}

The ability to perform an approximate nearest neighbor (ANN) search on the vector space (minhash sketch) enables researchers to quickly prune the search space of millions of samples to find the sample of interest. PIs  Bender, Pandey, and Patro have developed Mantis~\cite{PandeyAlBe18}, an inverted index on \kmers, to perform sequence-level searches. Our goal is to further reduce the index space by indexing sequences using sketches instead of \kmers and speed up the construction and query using GPUs on larger datasets.

\begin{rproblem}
Develop distributed algorithms that are close to the communication lower bound to  compute the similarity score for two set of items.
\end{rproblem}

\begin{rproblem}
Develop a distributed method to compute similarity score (Jaccard index) for sets of items.
\end{rproblem}

We plan to develop distributed algorithms that are close to the communication lower bound to estimate sequence similarities. Effectively, all linear sketching algorithms~\cite{li2014sketchuniversal} are highly parallelizable, easy to distribute, robust to arbitrarily ordered input streams, and have good data locality. In a nutshell, they trade more computation for a smaller space requirement. Our approach is to develop parallel and distributed techniques to compute sketches without communicating sequences across nodes but rather only communicating the hash sketches.

Another fundamental challenge related to sketching is cardinality estimation. In computational biology this arises as as attempt to estimate the cardianlity of the \kmer multiset from sequence files. 

\begin{rproblem}
Build a GPU-enabled cardinality estimator for \kmers in raw sequencing samples (genomic, transcriptomic, metagenomic).
\end{rproblem}

There is an optimal algorithm to compute the cardinality of the set~\cite{Kane2010}.
Our approach is to build a GPU-accelerated solution for cardinality estimation using known algorithms that are proven to be optimal.

\begin{rproblem}
Explore the feasibility of cardinality estimators that observe only a  subset of the data, and if promising, build such a GPU-enabled cardinality estimator for \kmers.
\end{rproblem}

Existing algorithms assume the entire data stream is observed, focusing on reducing the working set size. Since reading and parsing large genomic datasets is time-consuming, cardinality-estimation algorithms that don't require a full pass over the data can significantly improve performance.

% Existing algorithms assume that the entire stream of data is observed.  Known algorithms focus on reducing the size of the working set.  Given that simply reading and parsing very large genomic datasets can be time-intensive, cardinality-estimation algorithms that do not require a full pass over the data will provide a substantial performance benefit.

%To the best of our knowledge there are no cardinality estimation techniques that look only at a subset of the data. 

Our strategy is to develop efficient cardinality-estimation techniques that look only at a subset of the data. 
% It is an open problem to determine how well one can do when looking only at a small subset of the data. 
We will draw inspiration from related work of estimating properties such as entropy and cardinality when only a subset of the data are observed~\cite{valiant2017estimating}.  We will explore extending such algorithms, and evaluate how these approaches work in the context of estimating the cardinality of \kmer multisets in genomic data.  
