\begin{itemize}%[leftmargin=*,nolistsep]

\item \textbf{Taxonomic classification.} Taxonomic classification~\cite{wood2014kraken} helps to identify the microbial 
% \mfc{only microbial?} 
taxa present in large-scale metagenomic and microbiome datasets coming from complex biological and environmental samples by assigning a taxonomic label to each sequencing read. This plays an important role of many computational genomics pipelines for metagenomics projects, and is closely tied to the related problem of taxonomic abundance estimation (determining the taxonomic composition of a sequencing sample)~\cite{truong2015metaphlan2,skoufos2022agamemnon,wei2022kmcp}.

 

Methods for taxonomic classification of metagenomic data~\cite{wood2014kraken} often use hash tables to index the \kmer content of samples and quickly compare samples to prune the search space.
Furthermore, to save space, many solutions~\cite{wood2019improved} also employ approximate sketches (e.g., locality sensitive hashing (LSH)~\cite{roberts2004reducing, Marais2019}) to represent metagenomic data and perform similarity computations over sketches or reduced representations of the data~\cite{Shaw2023}.
Some approaches~\cite{kim2016centrifuge} employ compact string indexes such as the FM-index~\cite{ferragina2000opportunistic} to perform sequence comparison to determine exact matches among the pruned samples. There are many different tools for taxonomic classification.  Although they different in the details of their bottlenecks, they share some features: they are all bottlenecked around some aspect of $k$-mer analysis, especially around compression schemes for $k$-mer analysis.
%\mfc{which of these are bottlenecks?} \rob{to Martin's point/question --- there are \emph{many} different tools for taxonomic classification using different indexing strategies. Kraken (and Kraken2) are likely the most popular and use k-mer / minimizer hashing-based approaches. However, other pipelines like centrifuge and the recent Centrifuger use the FM-index or r-index. They provide different tradeoffs. Do we want to specifically lean into one here, or claim that we will tackle both?}.

 

\item \textbf{Raw sequence search.} Raw sequence search involves identifying all sequencing samples that contain a given query sequence in a database of raw sequencing data, such as SRA~\cite{kodama2012sequence}. A query is an arbitrary sequence, such as a gene transcript, or viral genome. Though common genomics processing pipelines extract and condense useful information (e.g., genomic variants, transcript abundance, etc.) from raw sequencing data, current solutions often eliminate unexpected signal or diversity (e.g., specific viral or microbial reads that might be present across many samples~\cite{Edgar2022}). 
%\mfc{to be clear: this is a false-negative issue, not a performance issue}

 

Methods for raw sequence search use \kmer-based indexing tools to build an inverted index from the \kmers to the underlying samples where the \kmer is present. Tools such as SSBT~\cite{solomon2016fast} build a tree of approximate \kmer indexes to quickly prune the search space of samples. Other tools include:  Mantis~\cite{PandeyAlBe18}, which builds an exact inverted index that uses compact hash tables; or Metagraph~\cite{Karasikov2020}, which uses a \kmer-index based on a succinct representation of the de Bruijn Graph~\cite{bowe2012succinct}.

 

In raw sequencing data, singleton \kmers are most likely caused by sequencing errors, yet they make up a large fraction of the data~\cite{solomon2016fast,MarccaisKi11}. These tools often use filters to weed out singleton \kmers.

%\mfc{I still don't know what the bottleneck is.}

%\mab{So far, the proposal does a great job of convincing people that we are experts. And that requires most of the real estate. What still needs to happen is (1) we need to convince the reader that actual pain is experienced, and (2) we need to convince the reader that we have a solution to this pain. Only a small number of additional sentences, but necessary ones. We haven't even mentioned GPUs as a possible solution.}

 

\item \textbf{Pangenomics.}
Pangenomics~\cite{garrison2018variation} involves representing the biological variation represented within and across populations of individual organisms belonging to a species or group.  The most popular approaches to pangenomics store the genome of a species as a sequence graph consisting of genomes of multiple individuals (or multiple major haplotype groups) instead of a single linear reference. A primary goal of pangenomic variation analysis is to avoid biases that arise when treating a single genome as the reference when identifying or comparing variants across samples in a population.

 

Existing tools for constructing a pangenomic graph use a combination of succinct data structures (e.g., compressed bit vectors~\cite{garrison2018variation}), space-efficient, dynamic hash tables, and string data structures~\cite{pandey2021variantstore}.

% \prashant{Talk about Metahipmer here.}

\end{itemize}

%\mfc{this sounds like software engineering.  why is there a technical problem?   are we just having them pay for us to build stuff that no one has gotten around to building?  Still no discussion of what the challenges are.}

Our goal  is to build tools for performing complex biological analyses at the scale of terabytes and petabytes, for datasets that are available today, and beyond, for the datasets of the future. For example, raw sequencing data from SRA~\cite{kodama2012sequence} is already at petabyte scale, metagenomic data from WA and Rhizo~\cite{hofmeyr2020terabase} are hundreds of terabytes, and pangenomic data from the 100,000 Genome Project~\cite{1002021100} are sequencing individuals at population scale. To quickly process and perform biological analysis on these data we will exploit the massive computing in modern GPUs (e.g., NVIDIA's Tesla V100 and A100) and also the distributed computing infrastructure of supercomputers (Perlmutter~\cite{perlmutter} and Summit~\cite{summit}).


\subsection{Taxonomic classification and abundance estimation of metagenomic data}
\label{subsec:tax-class}

\textbf{Problem definition.}
% The microbes that live in an environment can be identified by their combined genomic material, also called the metagenome. 
Metagenomic datasets contain sequencing reads from multiple species (and strains) that are present in the environment from which the sample was drawn. Metagenomes provides an opportunity to closely examine complex interactions, such as phage-host and metabolic dynamics~\cite{national2007new}, and to monitor for sequences of concern, such as potential pathogens.
%
Taxonomic classification~\cite{wood2014kraken} helps to identify the microbial taxa present in large-scale metagenomic datasets. Assigning taxonomic labels to sequencing reads, and the related challenge of quantifying the associated abundances of these taxa, are important steps in many computational genomics pipelines for metagenomics projects.

\noindent
\textbf{Importance.}
Metagenomic sequencing produces genomic data from a collection of species instead of an individual isolate. A primary challenges is then the the development of methods for identifying the species contained in these samples, and accurately estimating their associated abundances.
%
For example, read classification is important for \emph{de novo} metagenomics assembly, which attempts to reconstruct the DNA sequence of each organism present in the metagenomic sample without using a reference database before performing the actual assembly step~\cite{venter2004environmental,brady2009phymm,brady2011phymmbl,rosen2008metagenome,segata2012metagenomic}.  Likewise, the related task of taxonomic abundance estimation is a critical step in understanding how micriobial abundances change over time, between conditions of interest, or in response to a stimulus or environmental change~\cite{Kaehler2019,wei2022kmcp,skoufos2022agamemnon,BlancoMguez2023}. While highly-related, these tasks represent distinct challenges, as taxonomic read assignment asks for a deterministic assignment of each read to some taxon (perhaps at some aggregated taxonomic level), while abundance estimation asks for an estimate of the abundance of the different taxa in a sample, allowing the probabilistic or proportional allocation of individual sequencing reads.

\noindent
\textbf{Challenges.}
The task of read classification is far from straightforward.
A metagenomic sample can contain thousands of genomes with varying level of similarities and often occurring at vastly different abundances. Metagenomic datasets today easily range from hundreds of GBs to TBs~\cite{hofmeyr2020terabase}, e.g., the WA dataset contains a collection of marine microbial communities from the Western Arctic Ocean and consists of 822~GB of 2.5 billion reads in 12 samples~\cite{hofmeyr2020terabase}, making traditional string-matching-based tools like BLAST~\cite{altschul1990basic} infeasible for classification at scale.
%
%Although BLAST is one of the most sensitive metagenomics alignment methods, it is computationally intensive, making it infeasible to run on the millions of reads present in metagenomic sequencing studies.

\noindent
\textbf{State of the art.}
Many recently developed taxonomic classification tools~\cite{ames2013scalable, kim2016centrifuge, menzel2016fast, wood2014kraken, wood2019improved, dilthey2019strain,liu2018novel} use a supervised approach to assign taxonomic labels to reads. Target genomes from a reference database are organized along with their taxonomic labels and decomposed into their corresponding \kmers. Subsequently, input reads are decomposed into \kmers, and queried in the index to determine the most appropriate (specific) taxonomic placement of the read, placing ambiguous classifications at a coarser taxonomic level (e.g. order, phylum, etc.).

\noindent
\textbf{Gaps and requirements.}
The size of sequenced microbial genetic sequences presents a computational and memory challenge. For example, the popular RefSeq complete genomes (CG) database for microbial species, and the BLAST nt and nr databases for high-quality nucleotide and protein sequences, respectively, contain $\approx50$ and $\approx200$ million sequences. The memory requirements for popular tools can easily exceed 100~GB~\cite{simon2019benchmarking} (e.g. Kraken's databse~\cite{wood2014kraken}), especially when the reference data includes large eukaryotic genomes~\cite{meiser2017sequencing, knutson2017porcine}.
%
Further, the universe of microbial sequences is large and diverse. The vast search space often results in false positives as sequences can be matched against multiple taxa. Also, a large number of undiscovered microbial species can result in false negatives as these species are not present in the database.

Existing taxonomic classification tools are memory intensive during index construction, and often require more memory than available on many single-node machines. Furthermore, the classification operation is computationally intensive, making it difficult to scale to terabyte-scale datasets.
This challenge is further compounded by the exponential growth in recent years of the number of sequenced microbial genomes, meaning that the number of comparisons that need to be performed for new sequencing reads is huge and ever-increasing.
%
Finally, existing reference database indexes forgo updatability to save space. However, the ability to add new assembled genomes to an existing database is a critical feature to avoid the burdensome re-indexing process.


\begin{rproblem}[\textbf{Metagenomic classification at terabyte scale}]
Build a software tool to perform reference-based taxonomic classification of metagenomic reads for terabyte-scale metagenomic datasets. The software tool will support adding newly sequenced microbial genetic sequences to the database and updating the labels of existing ones without rebuilding the whole index.
\label{rprob:taxo-meta}
\end{rproblem}

% \mfc{there is no discussion of what we are actually going to do.  what hardware?  what tests?  how will we know that we succeeded?  How much of an improvement do we need in order to succeed?  What it will it mean to our target application community if we succeed?}


\subsection{Large-scale raw sequence search}
\label{subsec:lss}

\textbf{Problem statement.} Raw sequence search involves identifying all sequencing samples in a database of raw sequencing data such as SRA~\cite{kodama2012sequence,KatzSLKBO22} that contain a given query sequence. A query is an arbitrary sequence, such as a transcript. Raw sequencing datasets contain tremendous, often unannotated, information, such novel gene products or latent viral reads, that may be absent from reference databases.

\noindent
\textbf{Importance.}
The Sequence Read Archive (SRA)~\cite{kodama2012sequence}, the world’s largest database of sequences, hosts approximately 10 petabytes %(1016 bp) 
of sequence data and is growing by 
%at the alarming rate of 
10 TB per day.
%
The vast majority of publicly available sequencing data (e.g., the data deposited in the Sequence Read Archive, or European Nucleotide Archive (ENA)~\cite{CumminsAABDEGHH22}) exist in the form of raw, unassembled sequencing reads. The raw sequencing data encodes information which is lost during the assembly process, and only a small fraction of all the raw sequencing data is assembled, making the raw sequencing data critical to answer complex questions related to e.g. biological diversity and to support novel discoveries.
%
Enabling scientists to analyze existing raw data at scale will provide insight into ecology, medicine, and industrial applications~\cite{LeviRAE18}.
%
The genomics community established this database to enable sharing of data, but the computational barriers to search leave it separated from the people most qualified to analyze it.

\noindent
\textbf{Challenges.}
Many tools, such as BLAST~\cite{altschul1990basic} and its variants, have been designed to perform sequence-level searches over publicly available databases of assembled genomes and known proteins. %Much subsequent work has focused on how to extend tools such as BLAST to be faster, more sensitive, or both~\cite{XXX}. \mfc{This previous sentence is a jolt. what does that have to do with raw sequence data?}
However, such tools fail to scale to the much larger volume and distinct nature of raw (i.e. unassembled) data, for example, unlike searching a query against an assembled reference, long queries (e.g. genes) are unlikely to be present in their entirety as an approximate substring of any individual string of the raw input. As such, the raw sequencing data have mostly been rendered impervious to sequence-level search, substantially reducing the data's utility.
%
%There are a number of reasons that typical reference-database-based search techniques cannot easily be applied in the context of searching raw, unassembled sequences. One major reason is that most current techniques do not scale well as the amount of data grows to the size of the SRA (which today is $\approx4$ petabytes of sequence information). A second reason is that searching unassembled sequences means that relatively long queries (e.g., genes) are unlikely to be present in their entirety as an approximate substring of the input.

\noindent
\textbf{State of the art.}
Several schemes have been proposed that promise to search raw SRA while overcoming these challenges. Solomon and Kingsford~\cite{solomon2016fast} introduced the sequence Bloom tree (SBT) data structure and an associated algorithm that enables an efficient search over thousands of experiments. They re-phrasing the query in terms of \kmer set membership in a way that is robust to the unassembled nature of the raw data. The resulting problem is coined as the \emph{experiment discovery problem}, where the goal is to return all experiments that contain at least some user-defined fraction of \kmers present in the query string.
%
Mantis~\cite{PandeyAlBe18} (developed by PIs Pandey, Patro and Bender) showed how to solve the experiment discovery problem using a combination of the counting quotient filter~\cite{PandeyBJP17} and an inverted-index by efficiently mapping \kmers to the set of experiments in which they appear. Mantis is faster to index and query, while avoiding the positives exhibited by the SBT (and related approaches~\cite{SolomonK17,HarrisM20,BingmannBGI19}). 
%
PIs Pandey and Patro further continued the effort to compress the Mantis index to reduce memory requirements scale~\cite{AlmodaresiPFJP19,AlmodaresiPFJP20} out of RAM to storage devices. Other approaches, like MetaGraph~\cite{Karasikov2020}, that rely on succinct string indexes like those we propose to enable on the GPU in this work have also proven promising in tackling this challenge.

\noindent
\textbf{Gaps and requirement.}
Existing raw sequence search indexes suffer from the scalability challenge. Indexing raw data at the scale of thousands of samples is memory-intensive and indexes often take hundreds of GBs. 
%For example, the SBT index size for 2652 experiments consisting of short-read RNA sequencing runs of human blood, brain, and breast tissues is 97~GB\@. 
Scaling existing indexes to the scale of 100K SRA samples is not currently feasible on single-node machines and scaling to an approximation of the entire SRA is likely not possible on a single machine, even with improved representations. Our LSM-Mantis~\cite{almodaresi2022incrementally} index showed how to efficiently scale to external SSD storage devices using Bentley/Saxe's transformation~\cite{BentleyS80}. However, even LSM-Mantis could only index 40K experiments from the SRA on a single machine. These indexes are reaching the limits of single-node RAM and storage hardware.

%Furthermore, SBT-based indexes are approximate; the search results include false positives, which can quickly become problematic even if only 1\% of results are false positives, due to the sheer size of the search space.
%Mantis's index is exact and has no false positives. Recently, LSM-Mantis~\cite{almodaresi2022incrementally} showed how to efficiently scale the index to SSD storage devices using Bentley/Saxe's transformation~\cite{BentleyS80}. However, even LSM-Mantis could only index 40K experiments from the SRA\@. These indexes are reaching the limits of single-node RAM and storage hardware.

In addition to representing the exact (or approximate) \kmer sets, a key challenge in constructing a large-scale sequence search index is the ability to build a compact yet updatable color-class representation. The \emph{color} of a \kmer is the set of indexed samples in which it is present, and
since many \kmers share exactly the same patterns of occurrence across samples, one can easily retain only the distinct colors (termed a color-class).  In existing work, there is a tension between the size of the color-class representation, and the ability to add to or update this representation. Highly-compressed representations exist~\cite{Karasikov2020,Pibiri2023MacDBG,AlmodaresiPFJP19}, but are (semi-)static.  In this proposal, we will seek to leverage our work on building dynamic succinct data structure primitives to develop novel, compact, and \emph{updatable} color-class representations, drawing on ideas from our prior work in compact static representations~\cite{AlmodaresiPFJP19,Pibiri2023MacDBG}.
% \prashant{Add a discussion about representing color-classes.}

\begin{rproblem}[\textbf{Build a raw sequence search index at SRA scale}]
Build a search index over all (non access-controlled) raw experiments present in SRA and enable interactive sequence-level searches. Host the search tool publicly to make it available for researchers around the world.
% Researchers can quickly ask biological questions and get answers.
% For example: what if researchers wants to determine: if a new putative disease-related transcript appeared in other samples, if a new fusion event is common among samples with a given subtype, which samples contain a new unexpected bacterial contaminant.
\label{rprob:seq-search}
\end{rproblem}

\subsection{Pangenomics}
\label{subsec:pangenomics}

\textbf{Problem definition.}
Pangenomics~\cite{sherman2020pan} involves cataloging the genomes of individuals in a related population in the form of a sequence graph to preserve the variation across the individuals. This pangenome is used as the reference genome for the species and forms the basis of subsequent studies. Representing population variation (e.g. SNVs, indels, copy numbers, etc.) is critical to understanding relatedness, population diversity, and to avoiding reference bias during analysis and disease studies. These graphs represent a population-level reference, against which one can perform sequence-to-graph alignment. 

\noindent
\textbf{Importance.}
Much of the field of genomics revolves around the existence of reference genomes, which are sampled from `typical' individual within a species.  However, scientists have begun to realize the scale of limitations imposed by representing an entire population by a single reference (or a small handful). To better capture population diversity, we can use a `pangenome', a collection summarizing the genetic diversity observed across many sequencing experiments.
A primary goal of pangenomic variation analysis is to avoid biases that arise when treating a single genome as the reference when identifying or comparing variants across samples in a population.  The first pangenomes were developed for small, easy-to-sequence bacteria. Even in that context, pan-genomes provided novel scientific insights and contributed to our understanding of underlying differences in pathogenicity, virulence and drug resistance~\cite{sherman2020pan}.

\noindent
\textbf{Challenges.}
Cataloging the DNA from all individuals in a species is a daunting task.
Building a pangenome graph involves storing the genomic sequences of thousands individuals in the form of a sequence graph. Each path in the sequence graph follows the genome of an individual and has a unique coordinate system. A coordinate systems helps to identify the location of a variant in the genome of an individual. Due to the presence of insertions and deletions in genomes across individuals, each individual can have a unique coordinate system~\cite{pandey2021variantstore}.
During variant queries, we need to identify a variant with respect to an individual's coordinate system due to the absence of a reference genome.
A Pangenome consists of thousands of individuals and hence thousands of coordinate system in a single index. Indexing and querying the pangenome graph across thousands of coordinate system is computationally and memory intensive and requires scale up and scale out solutions~\cite{garrison2018variation,pandey2021variantstore}.

\noindent
\textbf{State of the art.}
Pangenomic studies are performed by storing the genomes of multiple individuals in a genome graph (also known as the sequence graph or variation graph). A genome graph is a directed, acyclic graph (DAG) $G = (N, E, P)$ that embeds a set of DNA sequences. It comprises a set of nodes $N$, a set of direct edges $E$, and a set of paths $P$. For DNA sequences, we use the alphabet \{A, C, G, T, N\}\@. Each $n_i \in N$ represents a sequence $seq(n_i)$. Edges in the graph connect nodes that are followed on a path. Nodes on a path are assigned positions based on the coordinate systems of sequences they represent.

% Efficiently indexing and storing a genome graph is a computationally and memory/space intensive process due to the presence of thousands of coordinate systems corresponding to the individuals.
Existing applications that index genome graphs~\cite{pandey2021variantstore,garrison2018variation} are designed to use compressed string indexes and off-the-shelf graph indexes. VG toolkit~\cite{garrison2018variation} is one of the most widely used tools to represent genomic variation data, and it also supports multiple coordinate systems. VG toolkit stores each sample path as a list of nodes in the graph and maintains a separate index corresponding to the coordinates of the reference and samples.
%
PI Pandey developed VariantStore~\cite{pandey2021variantstore}. It encodes genomic variation in a directed, acyclic variation graph and build a position index (a mapping of node positions to node identifiers) on the graph to quickly access a node in the graph corresponding to a queried position. It builds an inverted index from variants to the nodes in the graph to achieve space efficiency due to repeated variants and fast variant queries.

\noindent
\textbf{Gaps and requirement.}
Current pangenomic indexes do not scale to population-scale variation datasets such as 100,000 Genomes project~\cite{1002021100}. For example, VG toolkit stores each sample path as a list of nodes in the graph and maintains a separate index corresponding to the coordinates of the reference and samples. Storing a separate list of nodes for each sequence impedes the scalability of the representation for storing variation from thousands of samples. Moreover, variants are often shared among samples, so storing a list of nodes for each sample path introduces redundancy in the representation.
%
Existing pangenome indexes do not support adding new genomes to an existing index. In order to add a new genomic sample to the pangenomic index required rebuilding the complete index. For example, both VG toolkit and VariantStore are static indexes and do not support an efficient approach for adding new genomes to an existing index.

Finally, pangenomic indexes need to support fast variant queries and sequence-to-graph alignment. Achieving both these operations in a single index is a challenging task. Existing tools are optimized for one of these operations but not both. VG toolkit is designed for fast sequence-to-graph alignment and VariantStore for variant queries.


\begin{rproblem}[\textbf{Building a pangenomic index at population-scale }]
 Build a pangenomic index that can perform fast variation queries and sequence-to-graph alignment at population-scale datasets available today. We further want to support adding new genomic samples to an existing index without rebuilding the index.
\label{rprob:pangenomics}
\end{rproblem}
