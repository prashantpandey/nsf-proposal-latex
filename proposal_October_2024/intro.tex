%!TEX root =  proposal.tex


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% These are the general sections to include.  %
%                                             %
% You can alter some names, but follow the    %
% suggestions in the NSF guidelines.          %
%                                             %
% If spacing is tight, play with negative     %
% vspaces w/in the text to reduce whitespace. %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 1: Introduction    %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{intro}


%\mfc{we don't explain which data structures are the bottleneck.  We don't calculate how much faster they can potentially get -- for example, if we use GPUs -- , nor translate that into a concrete biological win.  We don't say what the challenge is to realizing this upside.}

%\mfc{in short, some things are great, but some things are missing.  And some things are strange -- like the very late mention of GPUs.}

% {\color{blue} Outline of an intro:
% \begin{itemize}
%     \item \sout{set up a target problem of data analytics, where we are going to use comp bio as an example [Prashant + Michael]}
%     \item \sout{talk about how many data pipelines end up using the same basic data structures.  forward pointer to how we work this out in detail in section 3 for 3 seemingly different/unrelated comp bio problems [Prashant + Michael]}
%     \item \sout{so the topic of this proposal is high-throughput HPC versions of these data structures [Prashant + Michael] 3 bullets 1/2 page}
%     \item \sout{argue that things are now at a scale where this will require GPUs [John]}
%     \item \sout{then talk about how GPUs have been great for regular computations, but these data structures are irregular [John]}
%     \item \sout{talk about the technical challenges of building these data structures at scale [John]}
%     \item \sout{talk about why we will succeed/what has change in the world so we can succeed [John] 1/2 - 1 page (for all four bullets in total) [deduplicate with rest of intro]}
%     \item \sout{talk about how we have clear application targets that will validate that our work is useful on actual data pipelines [Prashant + Michael] 1/4 page}
% \end{itemize}
% }

Our ability to generate, acquire, and store data has grown rapidly over
the past decade, transforming fields such as biology, cosmology, drug
discovery, etc.   
%As a result, increasingly complex and large-scale analyses are employed to gain insights into this data.  \mab{I cannot tell whether I like this previous sentence, but it seems out of place.}
For
instance, new and improved
massively parallel high-throughput sequencing (HTS) technologies~\cite{reuter2015high} are already producing petabyte-scale
datasets~\cite{kodama2012sequence}, and the NIH estimates that ``genomics
research will generate between 2 and 40 exabytes of data within the next
decade''~\cite{NHGRIDataScience}.
This is one of many instances where our
ability to generate data is outpacing the improvements in hardware speed, and
storage capacity, and the throughput of an analysis pipelines.
% Furthermore, increasingly complex and large-scale analyses are employed to gain insights into this data.
The ability to scale complex data analyses to these vast quantities of data ranks among the biggest challenges in the next decade.

As we argue in this proposal, a large number of diverse applications are bottlenecked on a surprisingly small number of core data structure: filters~\cite{PandeyAlBe18, solomon2016fast},
hash tables~\cite{solomon2016fast,almodaresi2022incrementally}, locality
sensitive hashing~\cite{Marais2019}, and compressed string
indexes~\cite{Almodaresi2018Pufferfish}. 
In \S\ref{sec:compbio}, we analyze several data analytic tasks and find that these data structures come up over and over again, which is not surprising: hash tables are key to much of computation, and filters are lossily compressed hash tables; locality sensitive hashing has extensive use wherever nearest neighbor computations are needed; and compressed string indexes are key to any application that  processes strings, be they biological or linguistic.
Any improvements to the throughput of these data structures will have a disproportionately large impact on many applications. 
Specifically, the scale of today's data is
such that two main problems arise: \textbf{(1)} some analyses are simply
no longer feasible with existing data structures and algorithms---for example,
despite the long-standing desire to build a sequence-level index for the entire
sequence read archive (SRA)~\cite{SRA},\footnote{SRA is a public repository for DNA sequencing data.} existing efforts have not reached this
scale~\cite{Karasikov2020, HarrisM20, SolomonK17, almodaresi2022incrementally,
AlmodaresiPFJP20,PandeyAlBe18} and \textbf{(2)} other common analyses are so computationally burdensome in time and/or space that they are infeasible without an extravagant outlay of hardware/cloud-compute resources.
Consequently, the scale of data is such that the analysis and discovery cycle is infeasible slow.  We are thus in a situation in
which, in many cases, compute has overtaken data generation as the predominant
experiment-related cost~\cite{Muir_2016} (apart from humans).

\textbf{This proposal takes  a vertical-stack approach to addressing these performance issues}, in that we: 
(1) identify a core set of four data structures, enumerated above;
(2) demonstrate that they lie at the heart of a variety of data-analysis workloads; (3) implement  
new parallel and distributed versions of these data structures to exploit the
massive compute on GPUs; (4) provide an API for developers to integrate these high-performance
and scalable data structures in applications; and (5) validate the performance and utility of our system though testing in real-world applications, specifically two core computational analytic kernels (k-mer analysis and sequence alignment), at the heart of three key computational biology applications (taxonomic classification, raw sequence search, and pangenomics); see Figure~\ref{fig1} in \S\ref{sec:compbio} for a representation of the relationship among the data structures, the analytic kernels, and the applications.  Thus, our approach spans a range of interrelated areas: 
\emph{design of data structures} (highly concurrent, dynamic, and distributed
data structures); \emph{systems} (scaling up using GPU acceleration);
\emph{performance engineering} (scaling out using distributed data
structures); and \emph{applications} (validation through computational biology applications). 

%
The PIs together form an interdisciplinary team of researchers across four
focus areas: systems, performance engineering, applications (computational biology), and the design and analysis of data structures. The team is taking a vertically integrated
systems/performance engineering/theory/applications co-design approach to explore four tightly
interconnected research modules. These research modules are structured from
bottom-up across the computing stack.


\paragraph{CPU speed and RAM capacity cannot keep up with data growth.} CPU
performance increases and RAM sizes are not keeping up with the explosive data growth in
modern applications across databases, machine learning, scientific computing, and (in the context of this proposal) computational biology~\cite{Chen:2021:TGS}. Although CPU performance is increasing at
2--25\%/year and single-node RAM sizes are increasing at 2--11\%/year, genomics
data, for example, is likely to double in size every 1.5 years~\cite{kodama2012sequence}. 
\emph{Thus, today's software tools will not scale with tomorrow's data.} 
Two common tasks in
computational biology pipelines illustrate the point: 
performing \kmer analysis and sequence alignment  on petabyte-scale raw sequencing data are not
possible on single-node shared-memory systems. 
In the post-Moore's-Law period,
performance gains will come from software, algorithms, and novel architectures rather than
traditional semiconductor scaling~\cite{leiserson2020there}. We can achieve massive scalability by
first designing data structures and algorithms to scale up using modern
accelerators such as GPUs and second scaling out by using distributed memory
in a high-performance environment.


\paragraph{GPUs for data pipelines.} With power constraints (continuing to be) the main bottleneck in increased
computing performance, the superior performance, performance growth, and
power-performance of GPUs~\cite{Dally:2010:GCT,Dally:2021:EOT} already makes
them increasingly attractive at all scales, from mobile devices to laptop and
desktop computation to the largest supercomputers. The recent and extremely
rapid rise of deep-neural-network training  for deep-learning
applications~\cite{Amodei:2015:DS2,Chetlur:2014:CEP,Coates:2013:DLW,Hannun:2014:DSU}---a
field led by NVIDIA GPUs and CUDA---has resulted in NVIDIA-GPU-equipped data
centers in nearly every large Internet company and NVIDIA GPUs in each of the
three major cloud-computing providers. As well, GPUs have made enormous inroads
into the largest-scale computations. GPU-centered machines dominate the top of
the biannual TOP500 list~\cite{top500:jun2024}, including 9 of the top 10 (\#1
has AMD GPUs; 155 total of the 186 GPU-centered machines have NVIDIA GPUs). In
systems that have high compute requirements, GPU hardware is now ubiquitous.

These advantages of GPU hardware are so significant that GPU systems are
ubiquitous even though the GPU software ecosystem is much more
cumbersome/underdeveloped than the CPU world. It is precisely this gap that we
address with our proposal. We believe that the GPU presents the best future
direction for large-scale computation, epitomized by the applications we
discuss in this proposal, and that solving the research problems involved in
designing and building high-performance dynamic GPU data structures at scale
are a critical component of this direction.


\paragraph{The Technical Challenge.} Today's most successful GPU applications focus on
highly parallel, regular workloads. 
A grand challenge in GPU computing---and the focus of significant research effort over the past decade (including by three of the PIs)---has been
\emph{irregular} and/or \emph{sparse} workloads. 
Such workloads are challenging to parallelize
effectively, and this difficulty is exacerbated at large scale.
Thus, part of the work in this proposal involves making GPU-optimized variants of  the four bottleneck data structures, filters, hash tables, locality sensitive hashing, and compressed string indexes.
%Data structures to address these workloads at scale are the focus of this proposal.
We see ample existing work
that addresses subsets of the overall challenge: regular GPU workloads at
scale, where industry has made significant progress (e.g., training deep neural networks~\cite{Dubey:2024:TL3}); irregular problems at modest scale (including the contributions of PI Pandey~\cite{DBLP:conf/ipps/NisaPEOBY21,DBLP:conf/ppopp/McCoyHY023} and PI Owens~\cite{Pan:2017:MGA,Chen:2022:SIP}), and complex dynamic GPU data
structures not at scale (including many contributions by all of the PIs of this
proposal)~\cite{Ashkiani:2018:ADH,Awad:2019:EAH,Junger:2020:WAL,Li:2021:DDH,mccoy2022high,Zhou:2021:DAD}.
The success of both of these thrusts of work gives us confidence
that we can build on them to address the grand challenge of dynamic and
distributed GPU data structures.






\paragraph{Our project.} In this project, we aim to build high-performance and scalable
data-analyses pipelines on the GPU that can apply to a wide variety of data-intensive
computations.  To do so, we will address the main computational
bottlenecks that are at the core of many existing systems by developing new
massively parallel and distributed data structures and algorithms for common
and widely needed data-processing tasks. 
As we argue in this proposal, we aim to address the fundamental problems described above, \emph{enabling certain heretofore intractable analyses}, and \emph{vastly reducing (by an order of magnitude or more) the key computational costs associated with some of the most common data analyses}; and
central to our approach is to leverage the massive parallelism of GPUs. 

% The challenge with (clusters of) GPUs  is that dynamic data structures have traditionally  been difficult to design and implement on GPUs, indeed, even on a single GPU\@.

We identify three representative analytical pipelines from computational biology to serve as our validating application targets.
%\mab{Can we say ``three separate applications in Comp bio, for some value of three? I can explain why verbally.}
Specifically, we identify two raw data analysis ``kernels'':
\Kmer analysis and sequence alignment. These kernels are used broadly
throughout sequence analysis tasks in computational biology. These are bottlenecks in many
workflows, so making them faster and scaling them to larger data
sets will have widespread impact. GPUs offer an avenue to scaling data
structures for these tasks. And while GPUs are fast, they are space-constrained.  Therefore,
we need new data structures and algorithms that are space-efficient to speed up
these applications. The scale of \Kmer analysis and sequence alignment applications are such that scale-out to multiple, distributed GPUs will be necessary. See
\Cref{sec:multigpu} for specifics.

While these workloads may exhibit ample parallelism and thus are potentially a
good fit for the GPU, they are challenging to implement efficiently because
they do not naturally map well to key GPU software design guidelines, including
\emph{minimizing thread divergence and contention, maximizing memory coherence,
and avoiding serialization and synchronization}. Furthermore, scaling out data
structures to a distributed domain adds the challenges of \emph{low-bandwidth,
high-latency communication and load balancing across GPUs}.

%We believe the time is right to address this problem with a multidisciplinary team with significant collaboration experience between the team members. 
Our recent successes in single-node and distributed GPU data structures and
algorithmic advances give us the
collective background to address the broad challenge of this proposal, and out expertise in computational biology give us a rich test bed to validate our advances.

We will build a standalone library of data structures optimized for single GPUs and for distributed clusters of GPUs.
We will test this library by evaluating them exhaustively on two concrete applications drawn from computational biology. 
A standalone library will enable easy integration and adoption of
these data structures in the applications from this proposal (and many others).

{Our approach is four-pronged.}  We will design scalable data structures; we will 
implement our solutions in a scale-up manner,  both on CPU and GPU\@; we will design distributed versions of our data structures and implement them on clusters of GPUS; and finally we will validate our solutions
on three {computational biology} workloads.  

We stress, however, that the data structures we have selected are general enough that they will find use both in other computational biology applications and in non-biological data analytic applications on large data.  

For example, with the ability to perform rapid seeding on the GPU (using the indices described herein), and taking advantage of our own expertise in building computationally efficient tools for the pre-processing of single-cell sequencing data~\cite{he2022alevin}, we believe that single-cell RNA-seq preprocessing will be an immediate ``low-hanging'' fruit that can be accelerated with our stack. It has already been demonstrated by NVIDIA that their RAPIDS single-cell~\cite{rapids} library can speed up steps of the downstream analysis of single-cell count matrices from 4.7--850$\times$. Yet, the preprocessing of raw data to obtain the count matrices to be analyzed, a computationally intensive process, does not yet have a GPU implementation. However, the required computation is embarrassingly parallel, with the processing of each sample requiring the alignment of hundreds of millions to billions of sequencing reads (highly parallelizable), followed by barcode correction (requires some synchronization) and UMI resolution (highly parallelizable as each cell can be evaluated independently). Thus, we anticipate implementing this preprocessing using our GPU-enhanced stack will see a one to two order-of-magnitude performance improvement over existing state-of-the-art solutions; a critical necessity given the incredibly rapid growth of single-cell sequencing data~\cite{scgrowth2022}.

% If successful we will be able to perform complex data analyses to answer biological questions on available terabyte- and petabyte-scale datasets, for example, raw sequencing data from SRA~\cite{kodama2012sequence}, metagenomic data from WA and Rhizo~\cite{hofmeyr2020terabase}, and pangenomic data from 100,000 genome project~\cite{1002021100}. \john{These cites / this argument is also made a page earlier; consider deleting this.} Existing data structures and software tools fail to scale to these data sizes, making these publicly available and highly valuable data resources largely inert.




% \prashant{Add a brief application about Single cell as another application of the library.}

% \subsection{Cross-cutting concerns}

% The cross-cutting concerns of particular interest to the PPoSS program are well-covered by this proposal.

% \begin{description}%[noitemsep,nolistsep]
%   \item[Scalability and performance] are our main evaluation criteria. Our explicit goals are to both increase the performance of our target applications on existing datasets and to increase the scale of datasets we can target beyond what is currently possible.

%   \item[Correctness and accuracy] are primary targets of our software development methodology (Section~\ref{sec:sw-methodology}), which describes our proposed use a suite of functional and performance tests; use of code-coverage tools together with these tests; and the use of continuous-integration and continuous-development tools and methodologies.

%   \item[The capabilities of heterogeneous architectures] are our chief motivation for our choice of single-GPU, multi-GPU, and multi-node-multi-GPU architectural targets for this work. We elaborate on this choice in Section~\ref{sec:gpu}.


%\end{description}
