%!TEX root =  proposal.tex

\paragraph{The Team.}
The team has a rare combination of skills needed for this proposal.  \defn{We are a tightly coupled team:}  All of the PIs have written multiple papers with other PIs, and we have 19 published papers, each authored by at least three  of the PIs.
%three of the PIs have published papers with at least three other PIs.  
\defn{The proposed work is in our main area of expertise:} Every PI has published in HPC, Systems, Computational Biology and Algorithms.  More specifically:
%One of the PIs is a world leader in GPUs, two of the PIs are world leaders in Computational Biology, two  of the PIs are world leaders in the theory of data structures, and two of the PIs are world leaders in high-performance data structures. All of us have as a main thrust of our research the transfer of theoretical results in data structures to practice, especially in HPC settings. Three of us have particular expertise in implementing sophisticated data structures in multi-node multi-GPUs settings.


\begin{itemize}%[noitemsep,nolistsep]
    \item \textbf{GPU Systems}: PIs Farach-Colton, Owens, and Pandey

    \item \textbf{High-Performance Computing}: PIs Bender, Farach-Colton, Owens, and Pandey

    \item \textbf{Large-scale computational biology} PIs Bender, Farach-Colton, Pandey, and Patro

        \item \textbf{Theory and Algorithms}: PIs Bender, Farach-Colton, Pandey, and Patro
    

\end{itemize}

The PIs also have a strong track record of technology transfer from other projects, and this places the PIs in a strong position to maximize the practical impact of the results from this project. 
%
Bender and Farach-Colton co-founded Tokutek, a database company, to commercialize their
NSF-supported research on streaming
B-trees. The products TokuDB~\cite{TokuDB} and TokuMX~\cite{TokuMX} were used in
production for cloud file system metadata, agile indexing in online
advertising, log-file analysis, on-line gaming, real-time analytics,
financial data, astronomy data, and genomic data.
PI Farach-Colton  built SplinterDB \cite{splinterdb, splinterdb2}, a high-performance key-value store that is deployed in production at VMware.
%PIs Bender and Farach-Colton founded and sold Tokutek~\cite{Tokutek14}, a database company that used write-optimized data structures to obtain order-of-magnitude improvements on database performance.

PI Bender's work on HPC  won an R\&D 100 Award for processor scheduling and allocation algorithms, which were licensed
by Cray and incorporated into SLURM.
PIs Bender, Farach-Colton, and Pandey applied techniques from the theory of write-optimized data structures to develop B$\varepsilon$trFS, a high-performance file system with provably good performance guarantees~\cite{JannenYuZh15a,JannenYuZh15b,login2,YuanZhJa16,login1,DBLP:conf/fast/0001CJMGBFJJPY20,DBLP:conf/hotstorage/ConwayKJBJJPF19,DBLP:journals/tos/ZhanJPCKFBYJJ18,DBLP:conf/fast/ZhanCJKBFJJPY18,DBLP:journals/tos/YuanZJPACDKWBFJ17,DBLP:journals/usenix-login/ConwayBJ0BJJKPY17,DBLP:conf/fast/ConwayBJJZYBJKP17} (this work won \textbf{Best Paper} at FAST'16 \cite{YuanZhJa16}). 
PIs Bender and Farach-Colton  applied their work on pointer compression to develop Mosaic Paging \cite{mosaicasplos, mosaicspaa}, a technique for increasing the coverage of hardware TLBs (won \textbf{Distinguished Paper} at ASPLOS'23 as well as \textbf{Top Pick} for IEEE MICRO 2023). This work will be deployed  in a not-yet-announced chip. 

Some of this tech transfer, such as PI Farach-Colton's 
Notung~\cite{ChenDuFa00}, and 
PI  Bender, Farach-Colton, Pandey, and Patro's counting quotient filters~\cite{PandeyBeJo17}, are  in computational biology. 
Counting quotient filters are incorporated into 
\mab{Prashant and Rob, approximately how many}
DNA sequencing tools.
Notung is a standard tool for reconciling phylogenetic trees.
%
PI Patro's developments in algorithms and data structures for compacted de Bruijn graph construction~\cite{Khan2021,Khan2022}, and lightweight-mapping~\cite{Srivastava2016,Almodaresi2018Pufferfish,srivastava2020alignment,Almodaresi2021} power the widely-use Salmon~\cite{Patro2017Salmon} tool for transcript quantification from bulk RNA-seq data, and the alevin-fry~\cite{he2022alevin,He2023} tool for gene quantification from single-cell RNA-seq data.  These tools are widely used throughout standard pipelines (like the nf-core RNA-seq pipeline) and databases (like the EMBL EBI single-cell expression atlas) within the community~\cite{Ewels2020,George2023}.

\mab{Prashant and Rob, put biology tech transfer. }
\mab{John and Prashant, please add GPU tech transfer. }

PI Owens led the first implementation of MPI on GPUs~\cite{Stuart:2009:MPO:withouturl,Stuart:2011:EMT}, the first multi-GPU MapReduce~\cite{Stuart:2011:MMO}, and more recent work on scalable graph analytics on HPC machines~\cite{Pan:2018:SBS,Pan:2017:MGA,Chen:2022:SIP}. PI Pandey has built the first GPU-based distributed-memory \kmer analysis pipeline for the MetaHipMer metagenome assembler~\cite{nisa2021distributed}.

\mab{This is text from elsewhere, but not targetted yet towards tech transfer: 
PIs Owens and Pandey have written numerous paper on GPU data structures and programming models. Owens' research program in GPU computing~\cite{Owens:2007:ASO,Owens:2008:GC} 
%spans nearly 20 years and 
includes representative research advances in fundamental algorithms~\cite{Sengupta:2007:SPF}, data structures~\cite{Lefohn:2006:GGE,Alcantara:2009:RPH}, 
    performance engineering~\cite{Zhang:2011:AQP}, programming models~\cite{Gupta:2012:ASO, Tzeng:2010:TMF}, and applications~\cite{Wang:2017:GGG}. Pandey's research includes building massively parallel and feature-rich GPU filters~\cite{mccoy2022high}, distributed-memory GPU hash tables for efficiently processing genomic data~\cite{nisa2021distributed}, GPU-accelerated exascale metagenomic assembly pipelines~\cite{McCoyHYP23a}, and high-performance and general-purpose GPU memory managers~\cite{McCoyP24}.
}

%\mfc{once we will the two paragraphs right before this subsection, we could put some of the stuff from the collab plan here.  Cuz it doesn't hurt to repeat part of that.}

%% We don't need the notions of scale anymore. It was specific to the PPOSS.
%\paragraph{Notions of Scale.}
% Our proposed work addresses several notions of scale.  First, our work involves scaling up data structures by designing them for GPUs. GPUs are cost-effective and offer massive parallelism, allowing significant speedups compared to CPUs. GPU data structures can help speed up computational-biology applications and quickly analyze large-scale datasets. Without a principled redesign of data structures, additional device RAM and GPU cores will be of diminishing value. Second, our work includes scaling out data structures in distributed memory across multi-node GPUs to quickly process petabyte-scale genomic and metagenomic datasets. This will involve building distributed data structures that can offer low communication volume and low load imbalance. Prior work has demonstrated that data movement and load imbalance is the major bottleneck for achieving high performance in a distributed application. Furthermore, computational biology datasets available today are already terabyte- and petabyte-scale. For example, raw sequencing data from SRA~\cite{kodama2012sequence}, metagenomic data from WA and Rhizo~\cite{hofmeyr2020terabase}, and pangenomic data from the 100,000 Genome Project~\cite{1002021100}.  To quickly process and perform biological analysis on these data, we need to exploit the massive computing in modern GPUs (V100 and A100) and also the distributed computing infrastructure of supercomputers (Perlmutter~\cite{perlmutter} and Summit~\cite{summit}).
