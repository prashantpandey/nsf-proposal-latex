\section{Research areas}

% The Project Description must contain a section titled "Research Areas" that must:

% Explicitly state and motivate at least four research areas covered (along with senior personnel with commensurate expertise);
% Describe the targeted distributed applications and systems, and the heterogeneous platforms on which they run; and
% Define relevant notions of scale and describe how scalability will be theoretically and experimentally evaluated for the targeted distributed applications and systems and heterogeneous platforms in (2) with respect to the full hardware/software stack.



\paragraph{Coverage areas.}
Data structures are ubiquitous throughout the hardware/software stack, as a way to build scalable computational biology applications that can process petabyte-scale genomic data.
Data structures have become a bottleneck, but redesigning data structures for GPUs to exploit massive parallelism and scaling to distributed memory is tantamount to renegotiating the division of labor among system components---requiring a closely knit team and an approach that weighs the costs and benefits holistically.
Our team has extensive expertise in the following four research areas:

% \begin{itemize}[noitemsep,nolistsep]

\begin{description}[noitemsep,nolistsep]
    \item[Theory and Algorithms (PIs Bender, Farach-Colton and Pandey)]
    Bender and Farach-Colton have written numerous related theory publications and have made significant contributions towards designing efficient data structures and building scalable applications using their theoretical contributions~\cite{BenderFaGo18,BenderFaJo12,PandeyBJ17,PandeyABFJP18Cell,PandeyBJP17a,PandeyBJP17b,ConwayFaSh18,JannenYuZh15a,JannenYuZh15b,YuanZhJa16,pandey2021terrace,pandey2021variantstore,pandey2022iceberght}.

    \item[GPU Systems (PIs Owens and Pandey)] Owens's research program in GPU computing~\cite{Owens:2007:ASO,Owens:2008:GC} spans nearly 20 years and includes representative research advances in fundamental algorithms~\cite{Sengupta:2007:SPF}, data structures~\cite{Lefohn:2006:GGE,Alcantara:2009:RPH}, % scalability to multiple GPUs~\cite{Stuart:2009:MPO,Stuart:2011:EMT,Stuart:2011:MMO,Pan:2017:MGA,Pan:2018:SBS,Chen:2022:SIP},
    performance engineering~\cite{Zhang:2011:AQP}, programming models~\cite{Gupta:2012:ASO, Tzeng:2010:TMF}, and applications~\cite{Wang:2017:GGG}. Pandey's research includes building massively parallel and feature-rich GPU filters~\cite{mccoy2022high} and distributed-memory GPU hash tables for efficiently processing genomic data~\cite{nisa2021distributed}.

    \item[High-Performance Computing (PIs Bender, Farach-Colton, Owens, and
        Pandey)] Bender and collaborator Cindy Phillips have
      written a number of top-tier related papers in HPC~\cite{pandey2020timely,bender2017two,eckstein2015pebbl,agrawal1989four,bender2008communication,greenberg1999enabling},
      and had considerable impact on HPC practice.
      PI Bender's and Phillips's work in HPC has focused on scheduling and  won a joint R\&D 100 Award for processor scheduling and allocation algorithms, which were licensed by Cray and incorporated into SLURM\@.  PIs Bender and Farach-Colton's company Tokutek deployed software to manage metadata in a large cloud storage service. Owens led the first implementation of MPI on GPUs~\cite{Stuart:2009:MPO:withouturl,Stuart:2011:EMT}, the first multi-GPU MapReduce~\cite{Stuart:2011:MMO}, and more recent work on scalable graph analytics on HPC machines~\cite{Pan:2018:SBS,Pan:2017:MGA,Chen:2022:SIP}. Pandey has built the first GPU-based distributed-memory \kmer analysis pipeline for the MetaHipMer metagenome assembler~\cite{nisa2021distributed}.

    \item[Large-scale computational biology (PIs Bender, Farach-Colton, and Pandey)] Pandey's work in computational biology is having a transformative impact, because he is essentially rebuilding key parts of genomic, transcriptomic, and pangenomics analysis tool-chains around data structures of his design and gaining significant performance improvements over state-of-the-art tools.
    This work is described in flagship computational biology conferences (RECOMB, ISMB, WABI) and journals (Cell Systems, Genome Biology, Bioinformatics, JCB)~\cite{PandeyABFJP18Cell,PandeyBJP17a,PandeyBJP17b,AlmodaresiPFJP19,AlmodaresiPFJP20,pandey2021variantstore,almodaresi2017rainbowfish,almodaresi2022incrementally}.  PI Farach-Colton has papers in genome assembly~\cite{Choi2003}, phylogeny construction~\cite{Farach97,Ambainis97,FarachKKM97,Farach1999, Cohen1997}, and string indexes~\cite{Farach97,Ambainis97}.

\end{description}


\paragraph{Notions of Scale.}
Our proposed work address several notions of scale.
% First, multi-core parallelism on CPUs is critical to efficiently scale data structure to exploit CPU compute.
First, our work involves scaling up data structures by designing them for GPUs. GPUs are cost-effective and offer massive parallelism, allowing significant speedups compared to CPUs. GPU data structures can help speed up computational-biology applications by orders of magnitude \john{this phrase ``orders of magnitude'' makes me nervous, I don't want to make reviewers nervous} and quickly analyze large-scale datasets. Without a principled redesign of data structures, additional device RAM and GPU cores will be of diminishing value.
Second, our work includes scaling out data structures in distributed memory across multi-node GPUs to quickly process petabyte-scale genomic and metagenomic datasets. This will involve building distributed data structures that can offer low communication volume and low load imbalance. Prior work has demonstrated that data movement and load imbalance is the major bottleneck for achieving high performance in a distributed application.
Furthermore, computational biology datasets available today are already terabyte- and petabyte-scale. For example, raw sequencing data from SRA~\cite{kodama2012sequence}, metagenomic data from WA and Rhizo~\cite{hofmeyr2020terabase}, and pangenomic data from the 100,000 Genome Project~\cite{1002021100}. \john{Pretty sure the previous sentence was already stated in the intro.} To quickly process and perform biological analysis on these data, we need to exploit the massive computing in modern GPUs (V100 and A100) and also the distributed computing infrastructure of supercomputers (Perlmutter~\cite{perlmutter} and Summit~\cite{summit}).
