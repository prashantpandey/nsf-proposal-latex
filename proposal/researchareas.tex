\section{Research areas}

\paragraph{Coverage areas.} Data structures are ubiquitous throughout the
hardware/software stack, as a way to connect heterogeneous components within a
system and to scale-out in data-center-scale applications.  AT has become a
bottleneck, but redesigning AT is tantamount to renegotiating the division of
labor among system components---requiring a closely knit team and an approach
that weighs the costs and benefits holistically.
Our team has the needed expertise:

% \begin{itemize}[noitemsep,nolistsep]
\begin{description}
  \item[Theory and Algorithms (PIs Bender and Farach-Colton)]
    Bender and Farach-Colton have written numerous
    top-tier theory
    publications~\cite{DBLP:conf/stoc/BenderFK19,DBLP:conf/focs/BenderFGJM018,DBLP:conf/soda/BenderCCFJT19,DBLP:conf/soda/AfshaniBFFGT17,DBLP:conf/pods/BenderFJMMPX17,DBLP:conf/stoc/BenderKPY16,DBLP:conf/soda/BenderFGKM17,DBLP:conf/pods/BenderBJKMPSSZ16}
    and have, along with collaborator Conway, made significant contributions in
    the theory of hashing and its
    applications~\cite{BenderFaGo18,BenderFaJo12,BenderFaJo11,PandeyBeJo17,PandeyAlBe18,PandeyBeJo18,PandeyBeJo17c,DBLP:conf/icalp/ConwayFS18}.


    \item[GPU Systems (PIs Owens and Pandey)] Owens's research program in GPU computing~\cite{Owens:2007:ASO,Owens:2008:GC} spans nearly 20 years and includes representative research advances in fundamental algorithms~\cite{Sengupta:2007:SPF}, data structures~\cite{Lefohn:2006:GGE,Alcantara:2009:RPH}, performance engineering~\cite{Zhang:2011:AQP}, programming models~\cite{Gupta:2012:ASO, Tzeng:2010:TMF}, and applications~\cite{Wang:2017:GGG}.

    \item[High-Performance Computing (PIs Bender, Farach-Colton, Owens, and
        Pandey)] Bender, Farach-Colton, Owens, and Pandey have
      written a number of top-tier papers in HPC~\cite{pandey2020timely,bender2017two,eckstein2015pebbl,agrawal1989four,bender2008communication,greenberg1999enabling},
      and had considerable impact on HPC practice.  PI Bender's and Phillips
      work in HPC has focused on scheduling and  won a joint R\&D 100 Award for
      processor scheduling and allocation algorithms, which were licensed by
      Cray and incorporated into SLURM.  PIs Bender and Farach-Colton's company
      Tokutek deployed software to manage metadata in a large cloud storage
      service. Owens led the first implementation of MPI on GPUs~\cite{Stuart:2009:MPO:withouturl,Stuart:2011:EMT}, the first multi-GPU MapReduce~\cite{Stuart:2011:MMO}, and more recent work on scalable graph analytics on HPC machines~\cite{Pan:2018:SBS,Pan:2017:MGA,Chen:2022:SIP}.

    \item[Large-scale genomics (PIs Bender, Farach-Colton, and Pandey)]

\end{description}


\paragraph{Notions of Scale.}  Our proposed work address several notions of
scale.  First, virtual memory is a major bottleneck for scaling individual
machines to larger memories and CPU counts.  Without a principled redesign of
VM, additional RAM and cores will be of diminishing value.  Second, our work
supports \emph{scale-out} cloud applications, which rely on naming, placing, and
translating accesses to resources that are distributed across multiple machines.
%Currently, if one needs to do complex operations on sensitive data in the
%cloud, there is no secure and sensible option other than scaling out across
%multiple SGX enclaves.
If successful, our work will improve the performance of each node, as well as
the working-set size each node can handle, potentially improving both by orders
of magnitude.  Third, this includes scaling from only CPUs, to {\em
heterogeneous} computing accelerators, such as GPUs and FPGAs, at rack-scale or
greater.  Prior work has demonstrated that data movement and address translation
in GPUs is a first-order performance issue~\cite{pichai:gpu,
power:gpummu,rossbach:ptask}. Further, cloud computing infrastructure, such as
Microsoft's Catapult, aggregate these accelerators and move data  at the
granularity of racks, not single nodes~\cite{putnam:catapult}.  These
accelerators are evolving rapidly and in ways that are hard to anticipate; thus,
a principled design is necessary to efficiently scale to new classes of future,
unknown compute hardware.
