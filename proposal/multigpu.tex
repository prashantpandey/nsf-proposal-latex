%!TEX root =  proposal.tex

\section{Multi-GPU Communication Management}
\label{sec:multigpu}

The gap between GPU-memory and inter-GPU or inter-node bandwidth is quite large and as a consequence, the primary challenge in most multi-GPU systems is to maximize inter-processor bandwidth. Traditionally, either latency or bandwidth limits performance, but GPU implementations, given the constraints of the GPU architecture and programming model, are typically fairly latency-tolerant and most often limited by bandwidth, particularly as these systems extend beyond one node (as they will in this proposal). %Under that broad umbrella, however, the implementation of GPU-to-GPU communication often offers a tradeoff between high-latency, high-bandwidth or low-latency, low-bandwidth.

What are the obstacles to maximizing bandwidth? The most common multi-GPU implementations follow a bulk-synchronous parallel~\cite{Valiant:1990:ABM} programming model. BSP is simple and a good match for the underlying GPU programming model. There are few multi-GPU codes that aren't BSP\@. But, it has significant disadvantages when the goal is the highest performance. Synchronization in modern multi-GPU systems is expensive, and growing, and BSP has large synchronization costs. BSP typically runs in phases of ``compute'' and ``communicate'' and thus enables little overlap between the two. As well, GPUs are fast at local compute, thus pushing the bottleneck even more to communication. Finally, GPU programs are nearly always written in a  monolithic way, integrating compute and communicate into one intertwined whole. There are no effective programming-model mechanisms to separate the concepts of communication and computation, in large part because of the lack of any vendor-supplied runtimes (e.g., MPI) that are separate entities from application-specific code. While modern GPUs have increasingly performant mechanisms to send and receive from other GPUs with minimal cost and overhead, neither the programming models nor the ability to write flexible network managers on the GPU have kept up.

We propose two directions for addressing these challenges in the context of applications for computational biology.

\paragraph{Asynchronous Algorithms for Computational Biology Applications}

Our first goal is algorithmic: How can we recast the traditionally synchronous algorithms that underpin our applications of interest into an asynchronous formulation? By this, we mean relaxing the ordering constraints imposed by a BSP implementation, by (for instance) allowing data-structure updates to occur during rather than after computation, by not requiring a global barrier between each phase of a BSP implementation, or by allowing older data as the input to compute steps with the knowledge that updates to that data can efficiently be integrated into the computation at a later time. We have significant experience with these algorithmic transformations in the context of graph analytics~\cite{Chen:2022:AAT,Chen:2022:SIP}. These ordering constraints are the reason BSP applications do not overlap compute and communicate and reducing or eliminating them can pay a significant performance dividend.

\begin{rproblem}
Develop asynchronous formulations of core computational-biology applications and data structures.
\end{rproblem}

\paragraph{Separate Application and Runtime}

In the CPU world, a programmer would write a multiprocessor application such as those we are considering in conjunction with a user-defined runtime such as MPI\@. MPI is powerful and open-source and, if the user needs more or different capabilities, can in principle be extended or changed. The GPU world has no such equivalent. The only runtimes are closed-source and provided by the vendor. Yet a user-defined runtime would be the ideal vehicle by which we could manage GPU-to-GPU communication, for instance trading off latency and bandwidth (statically or dynamically) by accumulating batches of data before sending, or implementing transparent user-specified compression, prioritization, or on-the-fly reordering of data.

PI Owens's research group has recently investigated user-specified runtimes in the context of memory management (memory allocation and garbage collection). What we have built is a separable user-defined runtime for memory management, built atop NVIDIA's Multi-Process Service (MPS)\@. MPS allows multiple processes to run simultaneously on one GPU; we run application and runtime components separately and have built a communication mechanism between them. It turns out in the case of memory management, factoring these components has significant performance benefits because each component takes half the GPU resources, but the more important benefits are modularity and functionality.

We propose to implement inter-GPU communication in our applications through a user-specified runtime.

\begin{rproblem}
Develop a user-specified runtime for communication that supports static and dynamic tradeoffs between latency and communication and can be extended, depending on the application, to support compression, reordering, and/or prioritization.
\end{rproblem}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
