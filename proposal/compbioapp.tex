%!TEX root =  proposal.tex

\section{Computational biology applications}


\subsection{Taxonomic classification of metagenomic data}

The microbes that live in an environment can be identified by the combined genomic material, also called the metagenome.
In metagenomics, we aim to identify and characterise the microorganisms that live in an environment, such as the human gut or a soil sample.
The study of metagenomes provides an opportunity to closely examine complex biological interactions, such as phage-host and metabolic dynamics~\cite{national2007new}.

Both short-read and long-read sequencing data are commonly used in metagenomics. 
The advantages of short-read data are high accuracy and low cost, which allow metagenomes to be sequenced at high depth and provide large amounts of high-quality data to the software.
On the other hand, the short read length can be a limitation  as less information in available in terms of the nucleotide composition.
Long reads offer the advantage of longer sequences capable of spanning long genomic repeats that are notoriously difficult to handle with short reads, at the price of higher error rates and higher sequencing costs.
Such high error rates can make it difficult for computational software to distinguish different species in a metagenomics sample.

Typically, a metagenomics pipeline consists of four main steps that serve as preprocessing for a variety of downstream applications: DNA extraction, DNA fragmentation, DNA sequencing, and read classification.
Read classification is a critical first computational step in any metagenomic analysis pipeline.
For example, read classification is critical for \emph{de novo} metagenomics assembly, which attempts to reconstruct the DNA sequence of each organism present in the metagenomic sample without using a reference database before performing the actual assembly step.

Nevertheless, the task of read classification is far from straightforward.
First of all, species can be similar and even share some genomic patterns, which makes the discrimination complex. 
Moreover, genomes are not uniformly present in the sample.
In general, there are a few species that are very abundant, while many other organisms have a low proportion of genomic material in the sample.

In the literature, metagenomic analysis tools can be broadly divided into binning (or clustering) and taxonomic classification tools~\cite{simon2019benchmarking, maric2020benchmarking}. 
Taxonomic classification tools are further divided into taxonomic binning and taxonomic profiling. Taxonomic profilers compute the relative abundances of taxa in a dataset without assigning taxonomic labels to individual reads. In practice, however, these methods are often used interchangeably, as one can compute the profile of a dataset based on the taxonomic labels of individual reads.

Binning tools~\cite{balvert2019ogre, wickramarachchi2020metabcc, van2015two, alanko2017framework, cleary2015detection, girotto2016metaprob, luo2019metagenomic, alneberg2014binning, wu2016maxbin, sedlar2017bioinformatics, HerathTTAH17, kelley2010clustering, kang2019metabat, strous2012binning} perform \textit{de novo} binning (i.e., without using a reference database) and use unsupervised approaches. 
These tools perform classification based on nucleotide composition and/or abundance profiles of reads.

On the other hand, taxonomic classification tools~\cite{ames2013scalable, kim2016centrifuge, menzel2016fast, wood2014kraken, wood2019improved, dilthey2019strain,liu2018novel} use a supervised approach to assign taxonomic labels to reads.
These tools build a database of known genomes and their taxonomic labels from the NCBI database. 
The database is typically an index that maps smaller subsequences ($k$-mers) to the list of taxa corresponding to the genomes in which the subsequences occur. 
During the classification phase, the input reads are decomposed into $k$-mers and queried in the index to determine the most appropriate taxon.

Using only the nucleotide content of individual sequences is proven to be suboptimal for both short and long read data.
Often short reads simply do not have enough information (due to their limited length) to distinguish sequences that originate from different species.
Long reads are often hindered by their error rates, which can lead to misclassification.
However, long-read classification tools that use databases of known genomes have shown good performance despite the high error rates.

Recently, there have been approaches that use connectivity information between sequences to perform classification.
OGRE~\cite{balvert2019ogre} uses the overlap information between reads in the form of an overlap graph and a pre-trained logistic regression model to perform binning.
GraphBin~\cite{mallawaarachchi2020graphbin} and GraphBin2~\cite{mallawaarachchi2020graphbin2}, on the other hand, use the connectivity information between contigs (longer sequences obtained after assembly) in the assembly graph and perform label propagation in a semi-supervised setting for classification.

\subsection{Large-scale raw sequence search}

\subsection{Pangenomics}



\subsection{MetaHipMer}

Metagenome assembly involves reconstructing long contiguous sequences ({\it
contigs}) of genetic material from short input {\it reads}. These reads are
strings of bases (the DNA alphabet A,C,G,T) of length 150 to 250 that are
produced by gene sequencing machines.  For metagenomes, these reads are
extracted from environmental samples (e.g. gut bacteria, or a soil sample) that
contain the genes of potentially thousands of microbes, existing at varying
abundances.  The reads are error prone (typically about 0.24\% error per base)
and sequencing is done multiple times to ensure every region of genetic material
is covered with some error free sequences.

In the approach used by MetaHipMer, the reads are first divided into overlapping
substrings of fixed length {\it k}, called {\it $k$-mers}, which are then used
to form a de Bruijn graph~\cite{CompeauPeTe11}. In a de Bruijn graph, the
vertices are $k$-mers and edges connect any two $k$-mers that have an overlap of
$k-1$ bases. These vertices are stored in a hash table that is distributed
across all the compute processes.  The size of the hash table is dependent on
the number of unique $k$-mers.  Traversal of the de Bruijn graph enables the
construction of the contigs (longer sequences).  This approach is more efficient
than an all-to-all alignment of the reads, which would be prohibitive for the
size of typical metagenome datasets (up to billions of reads).

Forward and backward extensions of the $k$-mer and the counts of those
extensions are also maintained in the hash table along with the $k$-mer.
Information regarding the extensions and their counts is critical to identifying
correct paths in the de Bruijn graph and requires 28 to 52 bytes (depending on
$k$) to store each $k$-mer.

To ensure accurate contigs, the $k$-mers that occur only once (singletons) are
treated as errors and dropped. In a typical set of metagenome reads, 70 to 80\%
of unique $k$-mers are singletons, but they still need to be stored and counted
in the distributed hash table. In the default MetaHipMer implementation, storing
the unique $k$-mers is the most memory intensive part of the computation and can
be roughly an order of magnitude larger than the input data.  The space required
to store the $k$-mers can be much larger than the size of the original raw
dataset (up to $10\times$ larger) as $k$-mers contain a lot of redundant
information due to their overlaps.

The distributed hash table in MetaHipMer is implemented as a collection of local
hash tables, one per process, with communication happening via UPC++ remote
procedure calls (RPCs). In the hash table insertion phase, the $k$-mers are
aggregated, dispatched over the network, and inserted in bulk into the local
hash tables, which are running on GPUs. Using GPUs boosts performance, but
further constrains memory (e.g. the Summit supercomputer~\cite{VazhkudaiDBG18}
has an aggregate of 96GB GPU memory and 512GB CPU memory per node).

MetaHipMer runs on distributed systems with multiple nodes, and each node could
have multiple GPUs, e.g. Summit has 6 GPUs per node. To utilize all the GPUs on
a node, MetaHipMer maps the UPC++ processes to the GPUs in a round robin
fashion, so multiple processes will share each GPU using the Nvidia
Multi-Process Service (MPS). On a Summit node there will be 42 processes (one
per core) and hence there will be 7 processes per GPU\@. Using the MPS has the
benefits of simplicity, because there is no inter-GPU communication required,
and it improves performance by increasing the utilization of the GPUs.

\Cref{fig:mhm-kmer} shows different parts of the $k$-mer analysis phase in
MetaHipMer. In the standard pipeline, all $k$-mers are counted in the hash table
and then a separate phase is required to purge all the singleton $k$-mers.

\subsection{\Kmer distribution}

\begin{wraptable}{r}{4.5in}
\centering
%\resizebox{\columnwidth}{!}{%
    \begin{tabular}{c | c | c | c | c | c}
    \toprule
    {\bf Dataset} & \multicolumn{5}{c}{\bf Percentage singleton $k$-mers} \\
    \midrule
    & $K=21$ & $k=33$ & $k=55$ & $k=77$ & $k=99$ \\
    \midrule
    WA &  66 & 73 & 76 & 78 & 78  \\
    Rhizo &  67 & 75 & 80 & 83 & 85  \\
    Tymeflies & 63 & 62 & 67 & 69 & 71 \\
    \bottomrule
    \end{tabular}
 %   }
    \caption{Distribution of singleton $k$-mers in metagenomic datasets with different values of $k$.}
    \label{tab:kmer-dist}
\end{wraptable}

\Cref{tab:kmer-dist} shows the distribution of singleton \kmers in three
different metagenomic datasets. Singleton \kmers form a majority fraction of
the total number of distinct \kmers. The distribution often depends on the
sequencing depth and the size of $k$. The larger value $k$ results in larger
fraction of singleton \kmers. This is due to the higher probability of seeing
an erroneous base in the \kmer given the higher value of $k$. The erroneous
bases result in singleton \kmers.

Weeding out singleton \kmers before inserting them in the hash table to count
is critical in any \kmer analysis phase to reduce the memory usage of the
counting phase. These singleton $k$-mers can also be pruned from the hash table
after the counting phase. However, that results in the high peak memory usage
and much slower running time.

Using a space-efficient filter to weed out singleton \kmers helps to reduce
the memory pressure on the counting hash table thereby reducing the peak memory
usage and increased run time. See~\cref{fig:mhm-kmer}.


