\section{Computational biology applications}

\subsection{MetaHipMer}

Metagenome assembly involves reconstructing long contiguous sequences ({\it
contigs}) of genetic material from short input {\it reads}. These reads are
strings of bases (the DNA alphabet A,C,G,T) of length 150 to 250 that are
produced by gene sequencing machines.  For metagenomes, these reads are
extracted from environmental samples (e.g. gut bacteria, or a soil sample) that
contain the genes of potentially thousands of microbes, existing at varying
abundances.  The reads are error prone (typically about 0.24\% error per base)
and sequencing is done multiple times to ensure every region of genetic material
is covered with some error free sequences.

In the approach used by MetaHipMer, the reads are first divided into overlapping
substrings of fixed length {\it k}, called {\it $k$-mers}, which are then used
to form a de Bruijn graph~\cite{CompeauPeTe11}. In a de Bruijn graph, the
vertices are $k$-mers and edges connect any two $k$-mers that have an overlap of
$k-1$ bases. These vertices are stored in a hash table that is distributed
across all the compute processes.  The size of the hash table is dependent on
the number of unique $k$-mers.  Traversal of the de Bruijn graph enables the
construction of the contigs (longer sequences).  This approach is more efficient
than an all-to-all alignment of the reads, which would be prohibitive for the
size of typical metagenome datasets (up to billions of reads).

Forward and backward extensions of the $k$-mer and the counts of those
extensions are also maintained in the hash table along with the $k$-mer.
Information regarding the extensions and their counts is critical to identifying
correct paths in the de Bruijn graph and requires 28 to 52 bytes (depending on
$k$) to store each $k$-mer.

To ensure accurate contigs, the $k$-mers that occur only once (singletons) are
treated as errors and dropped. In a typical set of metagenome reads, 70 to 80\%
of unique $k$-mers are singletons, but they still need to be stored and counted
in the distributed hash table. In the default MetaHipMer implementation, storing
the unique $k$-mers is the most memory intensive part of the computation and can
be roughly an order of magnitude larger than the input data.  The space required
to store the $k$-mers can be much larger than the size of the original raw
dataset (up to $10\times$ larger) as $k$-mers contain a lot of redundant
information due to their overlaps.

The distributed hash table in MetaHipMer is implemented as a collection of local
hash tables, one per process, with communication happening via UPC++ remote
procedure calls (RPCs). In the hash table insertion phase, the $k$-mers are
aggregated, dispatched over the network, and inserted in bulk into the local
hash tables, which are running on GPUs. Using GPUs boosts performance, but
further constrains memory (e.g. the Summit supercomputer~\cite{VazhkudaiDBG18}
has an aggregate of 96GB GPU memory and 512GB CPU memory per node).

MetaHipMer runs on distributed systems with multiple nodes, and each node could
have multiple GPUs, e.g. Summit has 6 GPUs per node. To utilize all the GPUs on
a node, MetaHipMer maps the UPC++ processes to the GPUs in a round robin
fashion, so multiple processes will share each GPU using the Nvidia
Multi-Process Service (MPS). On a Summit node there will be 42 processes (one
per core) and hence there will be 7 processes per GPU\@. Using the MPS has the
benefits of simplicity, because there is no inter-GPU communication required,
and it improves performance by increasing the utilization of the GPUs.

\Cref{fig:mhm-kmer} shows different parts of the $k$-mer analysis phase in
MetaHipMer. In the standard pipeline, all $k$-mers are counted in the hash table
and then a separate phase is required to purge all the singleton $k$-mers.

\subsection{\Kmer distribution}

\begin{wraptable}[15]{r}[0.01in]{4.5in}
\centering
%\resizebox{\columnwidth}{!}{%
    \begin{tabular}{c | c | c | c | c | c}
    \toprule
    {\bf Dataset} & \multicolumn{5}{c}{\bf Percentage singleton $k$-mers} \\
    \midrule
    & $K=21$ & $k=33$ & $k=55$ & $k=77$ & $k=99$ \\
    \midrule
    WA &  66 & 73 & 76 & 78 & 78  \\
    Rhizo &  67 & 75 & 80 & 83 & 85  \\
    Tymeflies & 63 & 62 & 67 & 69 & 71 \\
    \bottomrule
    \end{tabular}
 %   }
    \caption{Distribution of singleton $k$-mers in metagenomic datasets with different values of $k$.}
    \label{tab:kmer-dist}
\end{wraptable}

\Cref{tab:kmer-dist} shows the distribution of singleton \kmers in three
different metagenomic datasets. Singleton \kmers form a majority fraction of
the total number of distinct \kmers. The distribution often depends on the
sequencing depth and the size of $k$. The larger value $k$ results in larger
fraction of singleton \kmers. This is due to the higher probability of seeing
an erroneous base in the \kmer given the higher value of $k$. The erroneous
bases result in singleton \kmers.

Weeding out singleton \kmers before inserting them in the hash table to count
is critical in any \kmer analysis phase to reduce the memory usage of the
counting phase. These singleton $k$-mers can also be pruned from the hash table
after the counting phase. However, that results in the high peak memory usage
and much slower running time.

Using a space-efficient filter to weed out singleton \kmers helps to reduce
the memory pressure on the counting hash table thereby reducing the peak memory
usage and increased run time. See~\cref{fig:mhm-kmer}.


\subsection{Large-scale raw sequence search}

\subsection{Taxonomic classification of metagenomic data}

\subsection{Pangenomics}

