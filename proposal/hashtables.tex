\section{Hash tables}
\label{sec:hash-tables}

% \prashant{Notes for Martin/John.}

% \prashant{Scale up: We need hash tables with: fine-grained resizing on GPUs. Applications often do not know the number of items in advance. If they under size the hash table they fail at run time and if they oversize they waste memory.}

% \prashant{Scale out: We need hash table that can be distributed across multiple nodes in HPC clusters. However, when partitioning data across multiple nodes there are two challenges: First, we need to reduce the load variance across nodes. Second, we need to reduce the communication volume to while constructing the hash table and also to perform operations on the hash table. Operations can include: performing a join/intersection across two distributed hash tables.}

% \prashant{Forward and backward extensions of the \kmer and the counts of those
% extensions are also maintained in the hash table along with the \kmer.
% Information regarding the extensions and their counts is critical to identifying
% correct paths in the de Bruijn graph and requires 28 to 52 bytes (depending on
% $k$) to store each \kmer.}

% \prashant{HT is built for multiple values of $k$ across different iterations of the contig mapping. The HT is required to support variable-length keys ranging from 32-bits to 200 bits.}


% \prashant{HT use case in MetaHipMer}

% \prashant{In the approach used by MetaHipMer, the reads are first divided into overlapping substrings of fixed length-$k$, called \kmers, which are then used
% to form a de Bruijn graph. In a de Bruijn graph, the
% vertices are \kmers and edges connect any two \kmers that have an overlap of
% $k-1$ bases. These vertices are stored in a hash table that is distributed
% across all the compute processes.  The size of the hash table is dependent on
% the number of unique \kmers which is not known in advance. Currently, an approximation of the number of unique \kmers is used to size the HT. If it is an over approximation GPU device memory is wasted and if it is an under approximation then \kmers are arbitrarily dropped when HT becomes full resulting in dropped quality of the final result.}

% \john{I don't see any requirement here for stability. In fact I think stability is probably not what we want because we probably want to periodically load-balance, or at least explore the cost vs.\ benefit of load balancing as part of our work. So I don't think any stability stuff gets included.}

% \prashant{I think we need stability for local, single-GPU hash tables. With stability we can avoid data movement and hence less contention. So, stability is important to achieve high concurrency. For the distributed setup, yes, we need to move items for load balancing.}

% \john{Question for Prashant. Do we need to support deletes? If so we probably want to have a garbage collection effort.}

% \prashant{Yes, we need deletes. Garbage collection could be an efficiency way to support deletes. We can enter tombstones during deletes and garbage collect in a lazy manner. Important to achieve high concurrency.}

% \john{Here's the stuff I want to write up. (1) Build dynamic hash tables with variable-length keys. I don't know how to do that but I know it's not really been done on GPUs. One possibility: Hash the \kmers and use the hash as the key into the hash table. Of course we can also look to natively support variable-sized keys. (2) Overlapping communication and on-GPU-hash-table operations, so we don't have periods with all-compute-no-communicate and all-communicate-no-compute. (3) Explore tradeoff between better load balance and cost of load balance. (4) Explore tradeoff between more locality and reduced hash table efficiency (this is basically ``can we reduce overall communication by trying to keep/move hash table entries to their home nodes, and does that help overall performance''). (5) Communication management, in particular bundling up small communications into larger ones to achieve better bandwidth.}

% \prashant{All these points are great. Just to add one point.. we can say we will use locality-sensitive hashing and domain knowledge to smartly partition data to reduce communication volume.}

% \noindent\john{\hrulefill{}}

While building static single-GPU fixed-key-size hash tables can be done efficiently and with high load factors~\cite{Awad:2023:AAI}, scaling and improving the functionality of GPU hash tables to support the bioinformatics workloads in this proposal presents several significant research challenges. In particular, we must address three important and orthogonal problems:

\begin{rproblem}
  Supporting more functional \textbf{dynamic} hash tables that can incrementally add and delete keys and values
  \label{rprob:dynamic-gpu-hashtable}
\end{rproblem}

\begin{rproblem}
  Extending hash-table support for \textbf{variable-length keys}
  \label{rprob:variable-hashtable}
\end{rproblem}

\begin{rproblem}
  \textbf{Distributing these hash tables across multiple GPUs} for both increased capacity and increased performance
  \label{rprob:dist-hashtable}
\end{rproblem}

\subsection{Dynamic hash tables}

While a large body of research has addressed building and querying static GPU hash tables, work on dynamic GPU hash tables~\cite{Ashkiani:2018:ADH,Junger:2020:WAL,Li:2021:DDH,Zhou:2021:DAD} has been much less common. This previous work has shown the possibility of low-level data structures for supporting dynamic operations (our work~\cite{Ashkiani:2018:ADH} uses buckets of linked lists, each made of warp-wide slabs, as its fundamental atom) but this work is generally balanced (and achieves high performance) only when its bucket count and average bucket size are near-optimal. For some applications this may be acceptable. For the bioinformatics applications we target, we expect the amount of stored data in a hash table (e.g., \kmer storage) to vary widely over the runtime of one application, and existing methods are likely to perform poorly. In the CPU world, such rebuilding is common, and handled in the background, but no such runtime capability is available in the GPU world. As well, existing work does not automatically reclaim memory on deletes, which is important in applications (such as \kmer analysis) that feature both insertions and deletes.

\paragraph{Proposed approach}

We propose to address these deficiencies (rebuilding for optimal sizing and reclaiming memory) by leveraging NVIDIA's MPS (``MultiProcessing Service'') capability to simultaneously run two applications. One application, and the one that will take most GPU resources, will be the hash table and its umbrella application. The second will be a small ``runtime'' application that uses modest resources and will rebuild and reclaim memory alongside the main application. This can be done without programmer intervention, without stopping the entire application to rebuild the data structure or reclaim memory, and most importantly, can reduce register usage for the main application by offloading its operations into the runtime and thus improve aggregate performance.

\subsection{Variable-length keys}

\Kmer analysis requires the storage of \kmer keys of variable length (32--200 bits). Existing dynamic hash tables only handle fixed-length, word-sized keys. Fortunately, the size of these keys is bounded, and this aids our proposed solution.

\paragraph{Proposed approach}

We anticipate two directions of research that will allow us to provide the necessary support:

\begin{itemize}
  \item The GPU's high computational capability likely makes it tractable to perform a large number of arithmetic operations on each key (as we noted in our 2011 static-hash-table work~\cite{Alcantara:2011:BAE}, ``we can run up to 1000 instructions per memory update and remain memory bound''). Thus we will investigate computing a hash function (e.g., MD5) on each key and use the (fixed-size) hash value as the key. If necessary, we can also store the keys themselves: modern GPU memory managers (e.g., Ouroboros~\cite{Winter:2020:OVQ}) maintain multiple pools of memory, each of which satisfies requests of different sizes. Again because of the bounded size of \kmers, we believe we can specialize a memory manager to only a few pools and yet still maintain high memory load factors.
  \item We also will study the design of a cascade of hash tables, where keys in the first hash table are the first 32 or 64 bits of the \kmer and the value associated with each key is a second hash table, whose keys represent the next 32 or 64 bits, and so on. This strategy would be impractical for arbitrary-length keys but for \kmers, with their bounded lengths, it appears to be feasible.
\end{itemize}

\subsection{Distributed hash tables}

GPU memory is modest in size and to scale up, rather than pay the high communication cost of communicating between CPUs and GPUs through the slow PCIe bus, the most common approach is to scale \emph{across} GPUs using faster technologies like NVLink and Infiniband. However, naively partitioning the hash table across GPUs and performing fine-grained, synchronous access to it is unlikely to permit competitive performance.

\paragraph{Proposed approach}

Thus we propose several directions that can help mitigate the potential performance issues of partitioning. First, we will build our application in a chunked and pipelined way so that we can simultaneously compute hash table keys for chunk $n+2$, use inter-GPU communication to be fetching values for chunk $n+1$, and process the results for chunk $n$. Such an approach is necessary to best use our computation and communication resources. Second, we will need to bundle up small-grained communications into larger packets and only send those packets when they are full to best use the full bandwidth of the communication resources. Finally, we will investigate distributed hash table strategies that allow us to trade off (i) better load-balance vs.\ the cost of that load balance and (ii) better locality vs.\ the penalty in hash table efficiency (for instance, locality-sensitive hashing techniques and domain-specific knowledge to place pieces of the hash table where they will most frequently be used).    Essentially, a random partition will give us excellent load balance and efficiency, but will result in a maximal amount of communication. We propose to relax (rather, explore the consequence of relaxing) ideal load balance and ideal efficiency to reduce overall communication volume.


% \noindent\john{\hrulefill{}
% I don't find anything under here relevant.
% \noindent\hrulefill}

% Hash tables are a core data structure in many applications, including key-value
% stores, databases, and big-data-analysis engines, and are included in most
% standard libraries.  Hash-table performance can be a substantial bottleneck for
% many applications~\cite{NealZu21,FanAn13,MetreveliZe12}.


% We argue that two stricter criteria, \emph{referential stability} and \emph{low
% associativity} should be optimized to yield high performance on PMEM.  As we
% will see, these two goals seem to be at odds with each other, and part of the
% innovation  of our hash table design is that it simultaneously achieves both.
% Naturally, the third design goal for a high-performance hash table is
% \emph{compactness}, but compactness also seems at odds with referential
% stability and low associativity.

% A hash table is said to be \defn{stable} if the position where an element is
% stored is guaranteed not to change until either the element is deleted or the
% table is resized~\cite{sandersstability,originalstability,KnuthVol3}.
% Stability offers a number of desirable properties.  For example, stability
% enables simpler concurrency-control mechanisms and thus reduces the performance
% impact of locking.  Moreover, since elements are not moved, writing is
% minimized, which improves PMEM performance.
% expensive than reads~\cite{pmem-measurements}.

% The \defn{associativity} of a hash table is the number of locations where an
% element is allowed to be stored.\footnote{Associativity is often associated with
% caches that restrict the locations an item may be stored in.  Here we refer to
% \emph{data structural associativity}, which is a restriction on how many
% locations a data structure may choose from to put an item in, even on fully
% associative hardware.} The best known low-associative (DRAM) hash table is the
% cuckoo hash table~\cite{Pagh:CuckooHash,PaghRo01}.  In the original design, each
% element has exactly two locations in the table where it is allowed to be stored,
% meaning that the associativity is two.  Low associativity yields a different set
% of desirable properties---most importantly, it helps search costs. For example,
% searching for an element in a cuckoo hash table is fast because there are only
% two locations in the table to check.  In addition, low associativity can enable
% us to further improve query performance by keeping a small amount of metadata~\cite{pandey2022iceberght}.


% In combination, stability can be used to achieve high insertion throughput in
% PMEM, where writes are expensive, and low associativity can be use to achieve
% high query  performance.  Furthermore, we also show how stability enables
% locking and concurrency-control mechanisms to be simplified, leading to better
% multithreaded scaling and simpler designs for crash consistency.

% Unfortunately, there is a tension between stability and low associativity.  If a
% hash table has associativity $\alpha$, and elements cannot move once they are
% inserted, then an unlucky choice of $\alpha$ locations for $\alpha$ elements can
% block a $(\alpha+1)$st element from being inserted.  As $\alpha$ decreases, the
% probability of such an unlucky event increases.  Cuckoo hashing reduces the
% probability of these bad events by giving up stability via \defn{kickout
% chains}, which are chains of elements that displace each other from one location
% to another. Practical implementations~\cite{LiAn14} generally increase the
% number of elements that can be stored in a given location---and thus the
% associativity---to reduce the kickout-chain length and increase the
% maximum-allowed \defn{load factor}, i.e, the ratio of the total number of keys
% in the table to the overall capacity of the table.


% Similarly, there is a three-way tension between space efficiency, associativity,
% and stability.  For example, cuckoo hash tables can be made stable if they are
% overprovisioned so much that the kickout-chain length reaches 0.  Such
% overprovisioning directly decreases space efficiency, but it also increases
% associativity.  Linear probing hash tables are stable (assuming they use
% tombstones to implement delete) but, as the load factor approaches 1, the
% average probe length for queries goes up, increasing associativity.  Other
% open-addressing hash tables have a similar space/associativity trade-off.
% Chaining hash tables are stable, but they have large associativity and
% significant space overheads.  CLHT~\cite{david2015asynchronized} improves query
% performance despite high associativity by storing multiple items in each node,
% but this further reduces space efficiency.

% \iffalse
% \subsection{B-trees}

% The \btree(or B$^+$-tree\footnote{A \bplustree is a scan-optimized variant of
% \btrees that stores all data records in the leaves and only pivot keys in the
% internal nodes. The \bplustree is the widely implemented variant of the \btree
% in real-world applications as it supports faster range
% scans~\cite{mongodb,couchdb,scylladb,conway2020splinterdb,postgresql}. In this
% paper, we refer to \bplustree as the \btree.})~\cite{BayerMc72} has been the
% fundamental access path structure in databases and storage systems for over five
% decades~\cite{Comer79,graefe2010survey}. \btrees are an extension of
% self-balancing binary search trees to arbitrary fanouts (with more than two
% children per node). They store elements in each node in a sorted array.  Given a
% cache-line size $Z$~\cite{AggarwalVi88}, a \btree with $N$ elements and node
% size $B = \Theta(Z)$ supports the point operations \proc{insert} and \proc{find}
% in $O(\log_B(N))$ cache-line transfers in the I/O model~\cite{AggarwalVi88}.
% \btrees are one of the top choices for in-memory indexing~\cite{ZhangChOo15} due
% to their cache efficiency though they were initially introduced for indexing
% data stored on disk~\cite{BayerMc72}. In this paper, we study and improve the
% performance of in-memory \btrees.

% \btrees are especially popular in databases and file systems because they
% support logarithmic point operations (inserts and finds) and efficient range
% operations (range queries and scans) that read blocks of
% data~\cite{Knuth98,rodeh2013btrfs}.  They are also extensively used as the
% in-memory index in many popular databases such as MongoDB~\cite{mongodb},
% CouchDB~\cite{couchdb}, ScyllaDB~\cite{scylladb}, PostgreSQL~\cite{postgresql},
% and SplinterDB~\cite{conway2020splinterdb}.
% \fi
