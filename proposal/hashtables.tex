\section{Hash tables}
\label{sec:hash-tables}

There are work-efficient algorithms for building building static, space-efficient, single-GPU, fixed-key-size hash tables~\cite{Awad:2023:AAI}.  However, scaling and improving the functionality of GPU hash tables to support the application workloads in this proposal presents several significant research challenges. In particular, we must address three important and orthogonal problems:

%\john{There is no theory in this section at all.} \rob{It's \emph{not} dynamic, but we have a body of work that relies on minimal perfect hashing. It might be nice to add a MPHF (construction and query) as a research objective to this section.}

\begin{rproblem}
  Supporting more functional \textbf{dynamic} hash tables that can incrementally add and delete keys and values on GPUs.
  \label{rprob:dynamic-gpu-hashtable}
\end{rproblem}

\begin{rproblem}
  Extending hash-table support for \textbf{variable-length keys}.
  \label{rprob:variable-hashtable}
\end{rproblem}

\begin{rproblem}
  \textbf{Distributing these hash tables across multiple GPUs} for both increased capacity and increased performance.
  \label{rprob:dist-hashtable}
\end{rproblem}

These research problem are tailored to address issues in scaling hash tables for computational biology, and the solutions will be evaluated in that context.

\subsection{Dynamic hash tables}

While a large body of research has addressed building and querying static GPU hash tables, work on dynamic GPU hash tables~\cite{Ashkiani:2018:ADH,Junger:2020:WAL,Li:2021:DDH,Zhou:2021:DAD} has been much less common. This previous work has shown the possibility of low-level data structures for supporting dynamic operations (work~\cite{Ashkiani:2018:ADH} by PIs Farach-Colton and Owens uses buckets of linked lists, each made of warp-wide slabs, as its fundamental atom) but this work is generally balanced (and achieves high performance) only when its bucket count and average bucket size are near-optimal. For some applications this may be acceptable. For the bioinformatics applications we target, we expect the amount of stored data in a hash table (e.g., \kmer storage) to vary widely over the runtime of one application, and existing methods are likely to perform poorly. In the CPU world, such rebuilding is common, and transparently handled in the background, but no such runtime capability is available in the GPU world. As well, existing work does not automatically reclaim memory on deletes, which is important in applications (such as \kmer analysis) that feature both insertions and deletes.




\noindent\textbf{Proposed approach.}
One promising approach (for mitigating the effects of rebuilding for optimal sizing and to reclaim memory) is to leverage  NVIDIA's MPS (``MultiProcessing Service'') capability to simultaneously run two applications. One application, and the one that will take most GPU resources, will be the hash table and its umbrella application. The second will be a small ``runtime'' application that uses modest resources and will rebuild and reclaim memory alongside the main application. This can be done without programmer intervention, without stopping the entire application to rebuild the data structure or reclaim memory, and most importantly, can reduce register usage for the main application by offloading its operations into the runtime and thus improve aggregate performance.

Work by PIs Bender and Farach-Colton~\cite{iceberg,partitionhashing} explore theoretical approaches to performing fine-grain resizing of hash tables while minimizing the number of memory allocations.  Such approaches are a promising complement to the above for building resizable hash tables on the GPU.

\subsection{Variable-length keys}

\Kmer analysis requires the storage of \kmer keys of variable length (32--200 bits). Existing dynamic hash tables only handle fixed-length, word-sized keys. Fortunately, the size of these keys is bounded, and this aids our proposed solution.

\noindent
{\bf Proposed approach.}
We anticipate two directions of research that will allow us to provide the necessary support:

\begin{itemize}[noitemsep,nolistsep,leftmargin=*]
  \item The GPU's high computational capability likely makes it tractable to perform a large number of arithmetic operations on each key (as PI Owens noted in his 2011 static-hash-table work~\cite{Alcantara:2011:BAE}, ``we can run up to 1000 instructions per memory update and remain memory bound''). Thus we will investigate computing a hash function (e.g., MD5) on each key and use the (fixed-size) hash value as the key. If necessary, we can also store the keys themselves: modern GPU memory managers (e.g., Ouroboros~\cite{Winter:2020:OVQ}) maintain multiple pools of memory, each of which satisfies requests of different sizes. Again because of the bounded size of \kmers, we believe we can specialize a memory manager to only a few pools and yet still maintain high memory load factors.
  \item We also will study the design of a cascade of hash tables, where keys in the first hash table are the first 32 or 64 bits of the \kmer and the value associated with each key is a second hash table, whose keys represent the next 32 or 64 bits, and so on. This strategy would be impractical for arbitrary-length keys but for \kmers, with their bounded lengths, it appears to be feasible.
\end{itemize}

\subsection{Distributed hash tables}

GPU memory is modest in size and to scale up, rather than pay the high communication cost of communicating between CPUs and GPUs through the slow PCIe bus, the most common approach is to scale \emph{across} GPUs using faster technologies like NVLink and Infiniband. However, naively partitioning the hash table across GPUs and performing fine-grained, synchronous access to it is unlikely to permit competitive performance.

\noindent
{\bf Proposed approach.}
Thus we propose several directions that can help mitigate the potential performance issues of partitioning. First, we will build our application in a chunked and pipelined way so that we can simultaneously compute hash table keys for chunk $n+2$, use inter-GPU communication to be fetching values for chunk $n+1$, and process the results for chunk $n$. Such an approach is necessary to best use our computation and communication resources. Second, we will need to bundle up small-grained communications into larger packets and only send those packets when they are full to best use the full bandwidth of the communication resources. Finally, we will investigate distributed hash table strategies that allow us to trade off (i) better load-balance vs.\ the cost of that load balance and (ii) better locality vs.\ the penalty in hash table efficiency (for instance, locality-sensitive hashing techniques and domain-specific knowledge to place pieces of the hash table where they will most frequently be used).    
%We propose to relax (rather, explore the consequence of relaxing) ideal load balance and ideal efficiency to reduce overall communication volume. 
For example, domain-specific knowledge --- such as the fact that temporally proximate queries tend to be made for similar or overlapping sequences --- means that partitioning based on minimizer~\cite{roberts2004reducing} or other winnowing-related~\cite{Mar_ais_2018,DeBlasio_2019,Dutta_2022} schemes may work well for our purposes.It would seem that a random partition would give us excellent load balance and efficiency, but would result in a maximal amount of communication, though this has not been established, and we will study the exact shape of this tradeoff curve, both in theory and practice.

% \noindent\john{\hrulefill{}
% I don't find anything under here relevant.
% \noindent\hrulefill}

% Hash tables are a core data structure in many applications, including key-value
% stores, databases, and big-data-analysis engines, and are included in most
% standard libraries.  Hash-table performance can be a substantial bottleneck for
% many applications~\cite{NealZu21,FanAn13,MetreveliZe12}.


% We argue that two stricter criteria, \emph{referential stability} and \emph{low
% associativity} should be optimized to yield high performance on PMEM.  As we
% will see, these two goals seem to be at odds with each other, and part of the
% innovation  of our hash table design is that it simultaneously achieves both.
% Naturally, the third design goal for a high-performance hash table is
% \emph{compactness}, but compactness also seems at odds with referential
% stability and low associativity.

% A hash table is said to be \defn{stable} if the position where an element is
% stored is guaranteed not to change until either the element is deleted or the
% table is resized~\cite{sandersstability,originalstability,KnuthVol3}.
% Stability offers a number of desirable properties.  For example, stability
% enables simpler concurrency-control mechanisms and thus reduces the performance
% impact of locking.  Moreover, since elements are not moved, writing is
% minimized, which improves PMEM performance.
% expensive than reads~\cite{pmem-measurements}.

% The \defn{associativity} of a hash table is the number of locations where an
% element is allowed to be stored.\footnote{Associativity is often associated with
% caches that restrict the locations an item may be stored in.  Here we refer to
% \emph{data structural associativity}, which is a restriction on how many
% locations a data structure may choose from to put an item in, even on fully
% associative hardware.} The best known low-associative (DRAM) hash table is the
% cuckoo hash table~\cite{Pagh:CuckooHash,PaghRo01}.  In the original design, each
% element has exactly two locations in the table where it is allowed to be stored,
% meaning that the associativity is two.  Low associativity yields a different set
% of desirable properties---most importantly, it helps search costs. For example,
% searching for an element in a cuckoo hash table is fast because there are only
% two locations in the table to check.  In addition, low associativity can enable
% us to further improve query performance by keeping a small amount of metadata~\cite{pandey2022iceberght}.


% In combination, stability can be used to achieve high insertion throughput in
% PMEM, where writes are expensive, and low associativity can be use to achieve
% high query  performance.  Furthermore, we also show how stability enables
% locking and concurrency-control mechanisms to be simplified, leading to better
% multithreaded scaling and simpler designs for crash consistency.

% Unfortunately, there is a tension between stability and low associativity.  If a
% hash table has associativity $\alpha$, and elements cannot move once they are
% inserted, then an unlucky choice of $\alpha$ locations for $\alpha$ elements can
% block a $(\alpha+1)$st element from being inserted.  As $\alpha$ decreases, the
% probability of such an unlucky event increases.  Cuckoo hashing reduces the
% probability of these bad events by giving up stability via \defn{kickout
% chains}, which are chains of elements that displace each other from one location
% to another. Practical implementations~\cite{LiAn14} generally increase the
% number of elements that can be stored in a given location---and thus the
% associativity---to reduce the kickout-chain length and increase the
% maximum-allowed \defn{load factor}, i.e, the ratio of the total number of keys
% in the table to the overall capacity of the table.


% Similarly, there is a three-way tension between space efficiency, associativity,
% and stability.  For example, cuckoo hash tables can be made stable if they are
% overprovisioned so much that the kickout-chain length reaches 0.  Such
% overprovisioning directly decreases space efficiency, but it also increases
% associativity.  Linear probing hash tables are stable (assuming they use
% tombstones to implement delete) but, as the load factor approaches 1, the
% average probe length for queries goes up, increasing associativity.  Other
% open-addressing hash tables have a similar space/associativity trade-off.
% Chaining hash tables are stable, but they have large associativity and
% significant space overheads.  CLHT~\cite{david2015asynchronized} improves query
% performance despite high associativity by storing multiple items in each node,
% but this further reduces space efficiency.

% \iffalse
% \subsection{B-trees}

% The \btree(or B$^+$-tree\footnote{A \bplustree is a scan-optimized variant of
% \btrees that stores all data records in the leaves and only pivot keys in the
% internal nodes. The \bplustree is the widely implemented variant of the \btree
% in real-world applications as it supports faster range
% scans~\cite{mongodb,couchdb,scylladb,conway2020splinterdb,postgresql}. In this
% paper, we refer to \bplustree as the \btree.})~\cite{BayerMc72} has been the
% fundamental access path structure in databases and storage systems for over five
% decades~\cite{Comer79,graefe2010survey}. \btrees are an extension of
% self-balancing binary search trees to arbitrary fanouts (with more than two
% children per node). They store elements in each node in a sorted array.  Given a
% cache-line size $Z$~\cite{AggarwalVi88}, a \btree with $N$ elements and node
% size $B = \Theta(Z)$ supports the point operations \proc{insert} and \proc{find}
% in $O(\log_B(N))$ cache-line transfers in the I/O model~\cite{AggarwalVi88}.
% \btrees are one of the top choices for in-memory indexing~\cite{ZhangChOo15} due
% to their cache efficiency though they were initially introduced for indexing
% data stored on disk~\cite{BayerMc72}. In this paper, we study and improve the
% performance of in-memory \btrees.

% \btrees are especially popular in databases and file systems because they
% support logarithmic point operations (inserts and finds) and efficient range
% operations (range queries and scans) that read blocks of
% data~\cite{Knuth98,rodeh2013btrfs}.  They are also extensively used as the
% in-memory index in many popular databases such as MongoDB~\cite{mongodb},
% CouchDB~\cite{couchdb}, ScyllaDB~\cite{scylladb}, PostgreSQL~\cite{postgresql},
% and SplinterDB~\cite{conway2020splinterdb}.
% \fi
