\section{Sketches}

Measuring sequence similarity is at the core of many algorithms in computational biology~\cite{Myers2000, Langmead2012,Li2010}. Computing sequence similarity via optimal alignment is computationally intensive and requires an $O(n^2)$-time solution for two sequences of length $n$, in the worst case.  For highly divergent sequences, faster algorithms~\cite{MarcoSola2023} are known, and some have been implemented on the GPU~\cite{AguadoPuig2023}. However, due to the sheer size of datasets available today in terms of both complete and draft assemblies (e.g. metagenomic assemblies in RefSeq) and raw sequences in SRA, computing sequence similarities using traditional algorithms --- even the most efficient variants --- is not feasible.

Recently, researchers have employed dimensionality-reduction techniques, such as Minhash, ordered-min hash, and hyper log log, to include a pairwise mutation distance and P value significance test, enabling  efficient clustering of and searching in massive sequence collections~\cite{Mitzenmacher2014,ondov2016mash,Marais2019,Baker2019}. PI Pandey developed a new locality sensitive hashing (LSH) method, called Order Min Hash (OMH)~\cite{Marais2019}, for  edit distance. This method is a refinement of the minHash LSH used to approximate the Jaccard similarity, in that OMH is sensitive not only to the \kmer contents of the sequences but also to the relative order of the \kmers in the sequences. PI Pandey presented theoretical guarantees of the OMH as a gapped LSH\@.

One of the primary bottlenecks of these algorithms is the high computational cost of their linear sketching subroutines. Existing sketching techniques~\cite{ondov2016mash,Marais2019} are based on hashing and offer very poor locality. This makes them harder to scale up using GPUs.

\begin{rproblem}
Develop sketching algorithms that can replace hashing with batch sorting and achieve high performance on GPUs.
\end{rproblem}

Our approach is to develop algorithmic techniques where we can compute sketches by pre-computing and sorting the \kmers in a hash order. Doing this will help preserve locality and achieve massive parallelism on GPUs. PI Pandey has explored similar techniques in the past to preserve the order of \kmers in the sequence~\cite{Marais2019}.

\begin{rproblem}
Develop data structures for indexing and searching low-dimensional embeddings (based on minhash) for raw genomic and metagenomic data.
\end{rproblem}

The ability to perform an approximate nearest neighbor (ANN) search on the vector space (minhash sketch) enables researchers to quickly prune down the search space of millions of samples to find the sample of interest. PIs Pandey and Patro have developed Mantis, an inverted index on \kmers, to perform sequence-level searches. Our goal is to further reduce the index space by indexing sequences using sketches instead of \kmers and speed up the construction and query using GPUs on larger datasets.

\begin{rproblem}
Develop algorithms that are close to communication lower bound to compute the similarity score for two set of \kmers in a distributed setting.
\end{rproblem}

\begin{rproblem}
Design and build a system to compute similarity score (Jaccard index) for \kmer sets in a distributed setting.
\end{rproblem}

We plan to develop algorithms that are close to the communication lower bound to estimate sequence similarities in a distributed setting. Effectively, all linear sketching algorithms~\cite{li2014sketchuniversal} are highly parallelizable, easy to distribute, robust to arbitrarily ordered input streams, and have good data locality. In a nutshell, they trade more computation for a smaller space requirement. Our approach is to develop techniques to compute sketches in a distributed setting without communicating sequences across nodes but rather only communicating the hash sketches.

Another challenge in computational biology is to quickly and efficiently estimate the cardinality of the \kmer multiset from the sequence files. 


\begin{rproblem}
Build a GPU-enabled cardinality estimator for \kmers in raw sequencing samples (genomic, transcriptomic, metagenomic).
\end{rproblem}

There is an optimal algorithm to compute the cardinality of the set~\cite{Kane2010}.
Our approach is to build a GPU-accelerated solution for cardinality estimation using known algorithms that are proven to be optimal.

\begin{rproblem}
Explore the feasibility of cardinality estimators that observe only a  subset of the data, and if promising, build such a GPU-enabled cardinality estimator for \kmers.
\end{rproblem}

Existing algorithms assume that the entire stream of data is observed.  Optimal algorithms focus on reducing the size of the working set.  Given that simply reading and parsing very large genomic datasets can be a time-intensive operation, access to cardinality estimation algorithms that do not require a full pass over the data will provide a substantial performance benefit.

To the best of our knowledge there are no cardinality estimation techniques that look only at a subset of the data.   It is an open problem to determine how well one can do when looking only at a small subset of the data. We will draw inspiration from related work of estimating properties such as entropy and cardinality when only a subset of the data are observed~\cite{valiant2017estimating}.  We will explore extending such algorithms, and evaluate how these approaches work in the context of estimating the cardinality of \kmer multisets in genomic data.  
