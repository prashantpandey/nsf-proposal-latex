\section{Sketches}

Measuring sequence similarity is the core of many algorithms in computational biology~\cite{Myers2000, Langmead2012,Li2010}. Computing sequence similarity via optimal alignment is computationally intensive and requires an $O(n^2)$-time solution in the length of the sequences in the worst case. \draft{Since alignments between highly-divergent sequneces are usually not biologically meaningful, there has been an array of recent work that developed more efficient algorithms for the relevant case. This work has demonstrated that optimal alignment can be found in $O(s)$ memory (where $s$ is the score of the optimal alignment) and in $O(ns)$ time (where $n$ is the length of the shorter sequence)~\cite{MarcoSola2023}, and a somewhat simpler $O(s^2)$ memory version of this scheme has even been implemented on the GPU~\cite{AguadoPuig2023}.} However, due to the sheer size of datasets available today in terms of both complete and draft assemblies (e.g. metagenomic assemblies in RefSeq) and raw sequences in SRA, computing sequence similarities using traditional algorithms --- even the most efficient variants --- is not feasible.

Recently, researchers have employed dimensionality-reduction techniques such as Minhash, ordered-min hash, and hyper log log, to include a pairwise mutation distance and P value significance test, enabling the efficient clustering and search of massive sequence collections~\cite{Mitzenmacher2014,ondov2016mash,Marais2019,Baker2019}. PI Pandey developed a new locality sensitive hashing (LSH) method, called Order Min Hash (OMH)~\cite{Marais2019}, for the edit distance. This method is a refinement of the minHash LSH used to approximate the Jaccard similarity, in that OMH is sensitive not only to the \kmer contents of the sequences but also to the relative order of the \kmers in the sequences. We present theoretical guarantees of the OMH as a gapped LSH\@.

One of the primary bottlenecks of these algorithms is the high computational cost of their linear sketching subroutines. Existing sketching techniques~\cite{ondov2016mash,Marais2019} are based on hashing and offer very poor locality. This makes them harder to scale up using GPUs.

\begin{rproblem}
Develop sketching algorithms that can replace hashing with batch sorting and achieve high performance on GPUs.
\end{rproblem}

Our approach is to develop algorithmic techniques where we can compute sketches by pre-computing and sorting the \kmers in a hash order. Doing this will help preserve locality and achieve massive parallelism on GPUs. PI Pandey has explored similar techniques in the past to preserve the order of \kmers in the sequence~\cite{Marais2019}.

\begin{rproblem}
Develop data structures for indexing and searching low-dimensional embeddings (based on minhash) for raw genomic and metagenomic data.
\end{rproblem}

The ability to perform an approximate nearest neighbor (ANN) search on the vector space (minhash sketch) enables researchers to quickly prune down the search space of millions of samples to find the sample of interest. PIs Pandey and Patro have developed Mantis, an inverted index on \kmers, to perform sequence-level searches. Our goal is to further reduce the index space by indexing sequences using sketches instead of \kmers and speed up the construction and query using GPUs on larger datasets.

\begin{rproblem}
Develop algorithms that are close to communication lower bound to compute the similarity score for two set of \kmers in a distributed setting.
\end{rproblem}

\begin{rproblem}
Design and build a system to compute similarity score (Jaccard index) for \kmer sets in a distributed-setting.
\end{rproblem}

We plan to develop algorithms that are close to the communication lower bound to estimate sequence similarities in a distributed setting. Effectively, all linear sketching algorithms~\cite{li2014sketchuniversal} are highly parallelizable, easy to distribute, robust to arbitrarily ordered input streams, and have good data locality. In a nutshell, they trade more computation for a smaller space requirement. Our approach is to develop techniques to compute sketches in a distributed setting without communicating sequences across nodes but rather only communicating the hash sketches.

Another challenge in computational biology is to quickly and efficiently estimate the cardinality of the \kmer multiset from the sequence files. This requires computing the summary of the \kmer multiset by only looking at a small fraction of the sequence file and in limited memory. \rob{Actually, this is an interesting point. As far as I am aware, there are no cardinality estimation techniques that look only at a subset of the data.  That is, all existing cardinality estimation techniques in the comp bio space focus on keeping small working space, but must stream through the entire dataset in order to accurately estimate cardinality.  It is an interesting and (I believe) open problem of how well one can do when looking only at a small subset / prefix of the data.  Of course this requires distributional assumptions about the relationship between what is seen and what is unseen, but that is probably reasonable. Further, there is some work on this in the theory / ML community ``Estimating the Unseen''~\cite{valiant2017estimating}, but I don't know what the most recent results are here.}
%
The challenge of computing properties of massive data streams in limited space has inspired deep and beautiful literature on streaming algorithms and database systems. In the dynamic streaming model, the input is defined by a sequence of items of length $N$ and only $O(\polylog N)$ {RAM} is available for computation.


\begin{rproblem}
Build a GPU-enabled cardinality estimator for \kmers in raw sequencing samples (genomic, transcriptomic, metagenomic).
\end{rproblem}

There is an optimal algorithm to compute the cardinality of the set~\cite{Kane2010}.
Our approach is to build a GPU-accelerated solution for cardinality estimation using known algorithms that are proven to be optimal.

\begin{rproblem}
\draft{Explore the feasibility of cardinality esitmators that observe only a fixed subset of the data, and if promising, build such a GPU-enabled cardinality estimator for \kmers.}
\end{rproblem}

\draft{Existing optimal algorithms assume that the entire stream of data is observed, even if working memory is kept small.  However, there is also work into estimating properties like entropy and cardinality when only a subset of the data are observed~\cite{valiant2017estimating}.  We will explore extending such algorithms, and evaluate how these approaches work in the context of estimating the cardinality of \kmer multisets in genomic data.  Given that simply reading and parsing very large genomic datasets can be a time-intensive operation, access to cardinality estimation algorithms that do not require a full pass over the data will provide a substantial performance benefit.}
