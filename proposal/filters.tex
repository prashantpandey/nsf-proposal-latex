%!TEX root =  proposal.tex

\section{Filters}

A \defn{filter} is a data structure for maintaining a set $S\subseteq U$ under approximate membership queries.  Specifically, $\texttt{Member}(x)$ returns \textsc{True} if $x\in S$, and returns \textsc{True} with probability at most $\varepsilon$ for $x\notin S$, where $\varepsilon$ is a parameter of the filter specified at initialization.  When a filter returns \textsc{True} erroneously, we say that it has experienced a \defn{false positive}.

A filter is useful because it is small: for $n= |S|$ and $u = |U|$, an error-free dictionary requires $\Omega(\log {u\choose n})
\approx \Omega(n \log u)$ bits, whereas a filter  takes space at least  lower bound of $\opt + \Omega(n)$
bits~\cite{CarterFG78}.  There are several data structures that meet this lower bound w.h.p., including quotient filters~\cite{Cleary84,PaghPaRa05,DillingerM09,BenderFaJo12a,PandeyBJP17,PandeyCDBFJ21}, XOR
filters~\cite{GrafLe20} and Ribbon filters~\cite{DillingerW21}.  Bloom filters~\cite{Bloom70} do not meet this lower bound, nor, strictly speaking, do cuckoo filters~\cite{FanAnKa14,BreslowJ18}.\footnote{Cuckoo filters are interesting because they do meet the lower bound for reasonable size $n$, but as $n$ grows, the number of bits needed to build a cuckoo filter w.h.p. grows to $\Theta(n\log n)$.  In practice, however, cuckoo filters approximate $\opt + \Theta(n)$.}
%
For typical values of $\varepsilon$, the best filters takes one or two bytes per item.

The other way in which filters differ is their functionality. Some filters are static, in that they require $S$ to be given at initialization~\cite{GrafL19,DillingerW21}.  Some filters, such as Bloom filters, allow insertions but no deletions.  Quotient and cuckoo filters are fully dynamic.  Futhermore, quotient filters are \defn{mergable}, in that, under the right conditions, two filters can be merged into a single filter, perhaps increasing the false-positive rate, depending on how the filters are configured.  Mergeability turns out to be critical in some applications~\cite{conway2020splinterdb}.  Mergeability is closely related to \defn{resizability}.   Any filter can be resized to a larger capacity if we  simply rebuilt it from the set $S$.  We refer to a filter as resizable if it can increase its capacity without reference to $S$.  A filter is \defn{incrementally resizable} if we can efficiently resize it by factors of less than 2.  \mfc{Prashant: why do we need this feature?}

It is possible to turn some filters into \defn{maplets}, which are key-value filters.  That is, each element of $S$ has an associated value.  When querying for an element $x\in S$, one gets the associated value.  And whether $x\in S$ or not, one might get, with small probability, one or more extra values.  Maplets are critical to high-performance key-value stores, as well as \kmer analysis tools to maintain prefix-suffix extensions along with \kmers~\cite{mccoy2022high}. \mfc{I don't know hwat that means}  To date, all maplets are based on the quotient filter~\cite{conway2020splinterdb,mccoy2022high}.

Finally, some filters, most notably the quotient filter, can be made \defn{adaptive}~\cite{BenderFaGo18}.  A filter is adaptive if its false-positive probability is at most $\varepsilon$ for every $x\notin S$, even if $x$ is repeatedly queries.  In a standard filter, every time $x$ is queried, the filter returns the same answer.  In an adaptive filter, if $x$ is queried and determined to be a false positive, the filter has the option of adjusting itself so that this query is correctly answered as \textsc{False}.  \mfc{Prahsant, you said that this feature was needed in comp bio.  where and why?}

\prashant{Adaptive filters can be really useful for comp bio application as they can be used to construct MPH (minimal perfect hash) on GPUs.}

There are currently no maplets implemented for the GPU.  Our first presearch problem is thus:

\begin{rproblem}
Design and build a high performance maplet for the GPU.  
\end{rproblem}


\mfc{Not sure what the challenges are here}

\begin{rproblem}
Desing and build a high performormance incrementally resizable maplet.
\end{rproblem}
The naive way to resize a maplet is to double its size and move items (perhaps incrementally) from the old filter to the new.  This means that during during the transition, which can last quite a long time, both tables must be stored and both tables must be queried.  


\prashant{Comp bio applications require Maplets on GPUs that support dynamic resizing and deletes.}

\prashant{Adaptive filters can be really useful for comp bio application as they can be used to construct MPH (minimal perfect hash) on GPUs.}





% \textbf{Quotient
% filters}~\cite{Cleary84,PaghPaRa05,DillingerMa09,BenderFaJo12a,PandeyBJP17,pandeySigmod21}
% represent a set approximately by compactly storing small fingerprints of the
% items in the set via Robin Hood hashing~\cite{CelisLaMu85}. The quotient filter
% uses $1.053 (2.125 + \log_21/\epsilon)$ bits per element, which is less than the
% Bloom filter whenever $\epsilon \leq 1/64$, which is the case in almost all
% applications. It supports insertion, deletion, lookups, resizing, and merging.
% The counting quotient filter (CQF)~\cite{PandeyBJP17}, improves upon the
% performance of the quotient filter and adds variable-sized counters to count
% items using asymptotically optimal space, even in large and skewed datasets. In
% the counting quotient filter, we can also associate small values with items
% either by re-purposing the variable-sized counters~\cite{PandeyABFJP18Cell} to
% store values or by explicitly storing small values with the remainders in the
% table~\cite{PandeySMB20}.



% \textbf{Two-Choice filters}~\cite{pandeySigmod21} organize fingerprints
% compactly in blocks similar to the cuckoo filter. However, unlike the cuckoo
% filter, there is no kicking. The blocks in the two-choice filter are larger in
% size ($\approx \log{n}$, where $n$ is the number of items which is usually the
% size of the cache line on most machines) than the cuckoo filter and
% power-of-two-choice hashing is used to reduce the variance across the blocks and
% achieve a high load factor. During insertions if both blocks corresponding to a
% fingerprint are full then the data structure is declared full. The
% power-of-two-choice hashing enables the filter to probe exactly two cache lines
% during inserts and queries and write to a single cache line during inserts.
% Given the larger block sizes the vector quotient filter~\cite{pandeySigmod21}
% uses quotienting (similar to the quotient filter) to organize fingerprints
% inside blocks. It divides the fingerprints into a quotient and remainder part
% and only stores the remainder in the slot given by the quotient. It uses two
% additional metadata bits to resolve collisions among quotients.

\iffalse

\subsection{Analysis of filter designs}

We now look at the dynamic filters discussed in~\Cref{sec:prelim} and evaluate
them based on the GPU design principles.  Our goal is to identify the filters
that offer necessary features such as deletions, counting, and value
associations and at the same time satisfy most of the design principles.  We
will further discuss the challenges of implementing these filter on the GPUs to
achieve high speed operations.

Bloom filters are easy to implement on the GPU as they only require test and set
operations. These operations can be implemented using atomic operations and
achieve low thread divergence. However, each operation results in multiple cache
misses and therefore Bloom filters have low memory coherence. They also have
sub-optimal space usage. Moreover, Bloom filters do not support deletions,
counting\footnote{The counting Bloom filter~\cite{FanCaAl00}, a variant of the
Bloom filter, supports counting but it comes at a high space-overhead which
makes it highly inefficient in practice.}, and associating small values with
items.
% that many data analytics applications require.

Blocked Bloom filters are better suited to GPUs.  Each operation requires
probing inside a single block. They achieve low thread divergence, high memory
coherence, a high degree of parallelism, and atomic operations. Thus, blocked
Bloom filters can satisfy all the GPU design principles. However, blocked Bloom
filters have a high false-positive rate compared to Bloom filters and also do
not support necessary features like deletions and counting.

Operations in the quotient filter have high cache locality which makes it an
appropriate choice to achieve high memory coherence. However, insert operations
in the quotient filter requires shifting fingerprints which makes it harder to
use atomic operations and also results in high thread divergence. However, the
quotient filter can support all the necessary features like deletions, counting,
and associating small values with items which makes the quotient filter a highly
usable data structure that multiple applications can benefit from.

It is quite challenging to achieve high speed operations while maintaining all
of the features in a GPU implementation of the quotient filter. Geil et
al.~\cite{Geil:2018:QFA} implemented a preliminary version of the GPU quotient filter.
However, that implementation was adapted from Bender et al.'s quotient
filter~\cite{BenderFaJo12a}, which did not have all the features, like counting
and value association, and also had higher space overhead. Furthermore, Geil et
al.'s GPU-based quotient filter has implementation-specific limitations (e.g., it
supports a fixed false-positive rate and can only be sized to store less than
$2^{26}$ items) resulting in poor performance and limited scalability.

The cuckoo filter stores fingerprints in fixed size blocks. This design is
amenable to high memory coherence and low thread divergence. Atomic operations
can also be used to read and write fingerprints. However, the cascading sequence
of reads and writes to random memory locations makes the cuckoo filter hard to
implement efficiently on the GPU\@. In particular, at high load factors when the
number of kicked items becomes high, each insertion will result in very low
memory coherence. Moreover, each kicking operation results in multiple
cache-line writes. This makes it challenging to achieve high speed operations in
a GPU cuckoo filter. Moreover, cuckoo filters do not support counting and
associating small values with items.

The two-choice filter has the advantages of the cuckoo filter design. It has
fixed size blocks. Each operation requires probing into exactly two blocks, and
inserts and deletes only write into a single block. This results in low thread
divergence, high memory coherence, and a high degree of parallelism. However,
due to large block sizes a more sophisticated structure is required to maintain
fingerprints inside each block. Therefore, it is not straightforward to use
atomic operations to read or write fingerprints inside blocks. It is a
challenging task to implement a two choice filter on the GPU using atomic
operations to achieve high throughput.

\fi